---
title: Mandatory Assignment 2
author: Asbjørn Fyhn & Emil Beckett Kolko
date: 2024-04-019
execute:
  echo: false
  warning: false
  output: false
format:
  pdf:
    number-sections: true
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
jupyter: python3
---

**Introduction**

Jegadeesh and Titman (1993) documented that a portfolio that goes long high return stocks and short low return stocks over the past 3 to 12 months earns abnormal profits the following year. This phenomenon is known as the medium-term momentum effect. In this assignment, we first investigate this effect by estimating the returns of a long/short momentum portfolio using an ordinary least squares (OLS) regression. Next, we explore the predictions of cross-sectional stock returns using only past returns as predictors. This analysis is conducted through a machine learning framework that employs ridge regression. Instead of the positive correlation between last year’s returns and future returns found in Jegadeesh and Titman (1993), we find a negative correlation. Therefore, we conclude that a portfolio that goes long low return stocks and short high return stocks over the past 12 months earns abnormal profits.

**Exercise 1 and 2:**

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm
import tabulate

from plotnine import *
from mizani.formatters import percent_format
from mizani.colors.brewer.sequential import Blues as Blues
from regtabletotext import prettify_result

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.model_selection import (
  train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score
)
import os
```

```{python}
#Connecting to the database
tidy_finance = sqlite3.connect(
    database=f"../../data/tidy_finance_python.sqlite"
)

#Reading in crsp_monthly and factors_ff3_monthly
crsp_monthly = (pd.read_sql_query(
    sql="SELECT permno, month, ret_excess, mktcap, mktcap_lag FROM crsp_monthly",
    con=tidy_finance,
    parse_dates={"month"})
)

factors_ff3_monthly = pd.read_sql_query(
  sql="SELECT month, mkt_excess FROM factors_ff3_monthly",
  con=tidy_finance,
  parse_dates={"month"}
)
```

We load the "tidy_finance_python.sqlite" database and select variables from the tables "crsp_monthly" and "factors_ff3_monthly". Thereafter, we compute the momentum of stock i as described in the assignment.

When computing the momentum, we use the market capitalization as opposed to the stock price. The benefit of doing so is that our computations are not affected by events that increase or decrease the number of outstanding stocks, such as stock issuances or buybacks. If we had used the stock price in our computations as opposed to the market capitalization, such events would have artificially affected our calculated momentum.



```{python}
#Transforming the crsp_monthly dataset to contain mktcap_lag_12
mktcap_lag_12 = (crsp_monthly
  .assign(month=lambda x: x["month"]+pd.DateOffset(months=12))
  .get(["permno", "month", "mktcap"])
  .rename(columns={"mktcap": "mktcap_lag_12"})
  .dropna()
)

#Merging the datasets
data_for_sorts = (crsp_monthly
  .merge(mktcap_lag_12, how="inner", on=["permno", "month"])
)

#Computing the momentum of stock i for 12-month momentum
data_for_sorts['Mom_12'] = 100 * (data_for_sorts['mktcap_lag'] - data_for_sorts['mktcap_lag_12']) / data_for_sorts['mktcap_lag_12']
```

We present summary statistics of our dataset in @tbl-summary.

```{python}
#| output: true
#| label: tbl-summary
#| tbl-cap: Summary statistics of the dataset

# Summarizing the data for sorts with mean, std, min, 5%, 50% 95% and max
# List of variables to exclude
exclude_vars = ['month', 'permno', 'ret_excess']

# Filter out the excluded variables and displaying excess return in percentage terms
data_for_sorts_filtered = data_for_sorts.drop(columns=exclude_vars)
data_for_sorts_filtered['ret_excess_p'] = 100*data_for_sorts['ret_excess']

# Generate the summary statistics
data_for_sorts_summary = (
    data_for_sorts_filtered
    .describe(percentiles=[0.05, 0.5, 0.95])
    .round(2)
    .transpose()
)

data_for_sorts_summary
```
Where "ret_excess" is the monthly return above the risk free rate (1=1%), the "mktcap" variables are in USD millions, and "Mom_12" is the 12-month momentum in percentage terms (1=1%).

**Exercise 3:**

```{python}
#| output: true
#Bullet 1
#Creating the decile groups for the 12-month momentum

data_for_sorts['Mom_12_decile'] = pd.qcut(data_for_sorts['Mom_12'], q=10, labels=False) + 1

#Sorting the data by decile and month
sorted_data = data_for_sorts.sort_values(
    by=['Mom_12_decile', 'month'])

#Bullet 2
#Calculating the equal-weighted average values of Mom_12 and mc for each of the ten portfolios
average_return = (
    sorted_data
    .groupby(['Mom_12_decile'])
    .apply(lambda x: pd.Series({
        'Average_Mom_12': np.mean(x['Mom_12']),
        'Average_mktcap': np.mean(x['mktcap'])
    }))
    .reset_index()
)
# Specifiying the headers for the table, which we will use later to create table 2.
headers = {'Mom_12_decile':"Decile",
    'Average_Mom_12':"Average mom",
    'Average_mktcap':"Average mktcap",
}
average_return.rename(columns=headers,inplace=True)

for col in list(headers.values()):
  average_return[col] = average_return[col].astype(float)
```

The equal-weighted average values of the 12-month momentum and market capitalization are shown in @tbl-alphasacross. Furthermore, we present the average excess return and the CAPM alpha for the ten momentum-sorted portfolios in the same table (@tbl-alphasacross). The excess returns are reported in the "factors_ff3_monthly" dataset (note that they are reported as 0.01=1%, whereas we showed ret_excess_p as 1=1% in @tbl-summary). The CAPM alphas are estimated with OLS from the CAPM equation: 

$$r_{i,t}-r_{f,t}=\alpha_i+\beta_i(r_{m,t}-r_{f,t})+\epsilon_{i,t}$$

The null hypothesis is that alpha is zero and the alternative hypothesis is that alpha is different from zero:

$H_0: \alpha_i=0$ 

$H_A: \alpha_i\neq 0$

```{python}
#| output: true
# Bullet 3
# Computing the value weighted excess returns for each of the ten portfolios
Vw_excess_returns = (
    sorted_data.groupby(["Mom_12_decile"])
    .apply(
        lambda x: pd.Series(
            {
                "Decile_portfolio_excess_return": np.average(x["ret_excess"], weights=x["mktcap"])
            }
        )
    )
    .reset_index()
)

# Calculating the CAPM alpha for each of the ten portfolios
# Merging the data with the factors_ff3_monthly dataset
data_for_sorts = data_for_sorts.merge(
    factors_ff3_monthly, how="inner", on="month"
)
# Defining the linear regression function
def lin_reg(x,y):
    reg = sm.OLS(y, sm.add_constant(x)).fit()
    return reg.params["const"], reg.tvalues['const'], reg.pvalues['const']

# Running the CAPM regression for each of the ten portfolios
CAPM_alphas = (
    data_for_sorts.groupby(["Mom_12_decile"])
    .apply(
        lambda x: lin_reg(x["mkt_excess"], x["ret_excess"])
    )
    .reset_index()
)
CAPM_alphas['Alpha'] = [x[0] for x in CAPM_alphas[0]]
CAPM_alphas['t-statistic'] = [x[1] for x in CAPM_alphas[0]]
CAPM_alphas['p-value'] = [x[2] for x in CAPM_alphas[0]]
CAPM_alphas.drop(columns=[0], inplace=True)


# Merging the calculations to one dataset to present it in a table
Alphas_excessret = Vw_excess_returns.merge(
    CAPM_alphas, how="left", on="Mom_12_decile"
)

# Specifiying the headers for the table
headers_v2 = {'Mom_12_decile':"Decile",
    'Decile_portfolio_excess_return':"Excess ret",
}
# Renaming the headers
Alphas_excessret.rename(columns=headers_v2,inplace=True)

# Putting headers in place
for col in list(headers_v2.values()):
  Alphas_excessret[col] = Alphas_excessret[col].astype(float)


merged=pd.merge(average_return,Alphas_excessret,on='Decile',how='outer')#.set_index('Momentum decile')
```

```{python}
#| output: true
#| label: tbl-alphasacross
#| tbl-cap: Alphas across the ten portfolios

# Specifying decimals before printing the table
formatted_dfAgg_v2 = merged.style.format({
    'Decile': '{:.0f}',
    'Average mom': '{:,.2f}',
    'Average mktcap' : '{:,.0f}',
    'Excess ret': '{:.4f}',
    'Alpha': '{:.4f}',
    't-statistic': '{:.2f}',
    'p-value': '{:.2f}',
    'na_rep': "" 
})
formatted_dfAgg_v2.hide()
formatted_dfAgg_v2
```

We see from the t-statistics and p-values, that we can reject the null hypothesis in all cases except for the portfolio of 4th decile momentum stocks. As such, alpha is significantly different from zero in all other cases. To get a better sense of the distribution of alpha across the ten portfolios, we present the data in @fig-capmalphas.

```{python}
#| output: true
#| label: fig-capmalphas
#| fig-cap: CAPM alphas of momentum-sorted portfolios

# Plotting the alphas on the momentum portfolios
plot_momentum_portfolios_summary = (
  ggplot(Alphas_excessret, 
         aes(x="Decile", y="Alpha", fill="Decile")) +
  geom_bar(stat="identity") +
  labs(x="Decile", y="CAPM alpha") +
  scale_x_continuous(breaks=range(1, 11)) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
    theme(legend_position="none",
          figure_size=(6, 4))
)
plot_momentum_portfolios_summary.draw()
```

We see that the lowest decile stocks offer high alphas compared to the rest of the stocks. We expected the lowest performing stocks to have a negative alpha in accordance with Jegadeesh and Titman (1993). The 2nd and 3rd decile stocks are more aligned with their paper, showing negative alphas, and the stocks in the 5th decile and up offer positive alphas. These findings represent an imperfection in the market. In the following, we test if we can exploit this imperfection with a momentum strategy that goes long past winners and short past losers.

Specifically, we examine the alpha and beta of a portfolio that shorts the 1st decile portfolio and goes long the 10th decile portfolio. We note that this entails shorting stocks with high alpha to buy stocks with lower alpha, which seems counter-intuitive. We analyze the alpha and beta of this portfolio with t-statistics. Here our hypothesises are:

$H_0: \alpha=0$ 

$H_0: \beta=0$

To compute the Newey-West standard errors we must choose a bandwidth based on lags for the estimation. The choice here is rather arbitrary and not data-driven, but we choose a lag length of 12 months, since that is also the lag length of the momentum stocks. Our results are presented in @tbl-performance:

```{python}
# Naming 1 decile portfolio as low and 10 decile portfolio as high
data_for_sorts['portfolio'] = np.where(data_for_sorts["Mom_12_decile"]==1,'low',np.where(data_for_sorts["Mom_12_decile"]==10,'high','neutral'))

# Pivoting the data to get the long-short momentum portfolio
mom_ls = (data_for_sorts
          .query("portfolio in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio", values="ret_excess")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )
# Running the CAPM regression for the long-short momentum portfolio
model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1 + mkt_excess", 
    data=mom_ls
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)
```

```{python}
#| output: true
#| label: tbl-performance
#| tbl-cap: Performance of the momentum strategy
# Format the DataFrame style
output = (pd.DataFrame(index=['alpha','beta'],
                        data=({
                            'Estimate':model_fit.params.values, # parameters
                            'Std. Error': model_fit.bse.values,  # standard errors
                            't-Value':model_fit.tvalues.values, # t-values
                            'p-Value':model_fit.pvalues.values, # p-values
                            })))
output
```

We cannot reject the null hypothesis that the return, $\alpha$, of the momentum strategy is 0. This finding is in accordance with the CAPM model, which states that arbitrage strategies are not possible. However, our results deviate from Jegadeesh and Titman (1993) who suggest that it is possible to obtain a positive $\alpha$ with the momentum strategy. The deviation in our results can be explained by the high alphas of portfolio 1, which are not usually observed.

Our results show a significant negative $\beta$, which means the momentum (long/short) portfolio is negatively correlated with the market. This type of portfolio can be used to reduce the market risk of other investments with positive betas.

The returns of the portfolios are plotted in @fig-annualreturns
```{python}
#| output: true
#| label: fig-annualreturns
#| fig-cap: Annual returns of momentum portfolios

# Calculate annual returns for the 'low', 'high', and 'long_short' portfolios and reshape data for plotting
momentum_longshort_year = (mom_ls
  .assign(year=lambda x: x["month"].dt.year)
  .groupby("year")
  .aggregate(
    low=("low", lambda x: 1-(1+x).prod()),
    high=("high", lambda x: 1-(1+x).prod()),
    long_short=("long_short", lambda x: 1-(1+x).prod())
  )
  .reset_index()
  .melt(id_vars="year", var_name="name", value_name="value")
)
# Create a bar plot of annual returns
plot_momentum_longshort_year = (
  ggplot(momentum_longshort_year, 
         aes(x="year", y="value", fill="name")) +
  geom_col(position='dodge') +
  facet_wrap("~name", ncol=1) +
  labs(x="", y="") +
  scale_color_discrete(guide=False) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
  theme(legend_position="none",
          figure_size=(6, 4)))
plot_momentum_longshort_year.draw()
```

The graphs generally show an expected pattern with both positive and negative returns. All portfolios exhibit autocorrelation in the returns, which supports the argument for a momentum portfolio. Interestingly, the momentum long/short portfolio delivers higher returns more consistently than both the low and high momentum portfolio which have positive alphas. 

**Exercise 4**

We aim to estimate a model where a stock's excess return is linearly dependent on its own 60 previous lags. The regression model is as follows:

$$r_{i,t+1} = \sum_{k=1}^{60} b_k r_{i,t-k} + \sum_{k=1}^{60} c_k r_{i,t-k}^2$$

The squared term represents the magnitude of past returns, irrespective of their direction (positive or negative).Therefore, the coefficient $c_k$ captures how much the volatility (or magnitude of returns) from k periods ago affects the current period's excess return.

To estimate the coefficients of this model, we will use ridge regression, a regularized version of OLS regression. Ridge regression helps reduce the variance of the estimates by adding a penalty term to the OLS regression.This penalty term is the sum of the squared coefficients multiplied by a constant, $\lambda$, a hyperparameter that must be chosen. The optimal value of $\lambda$ is found through cross-validation. The regression coefficient has a closed-form solution:

$$\hat{\beta} = (X^TX + \lambda I)^{-1}X^Ty$$

Here $\hat{\beta}$ represents the estimated coefficients, $X$ is the matrix of predictors, $y$ is the vector of the dependent variable, and $I$ is the identity matrix.

First, we clean the dataset by removing the smallest stocks, specifically those with the lowest 20% market capitalization.

```{python}
# effectively not important copy. just to create better overview
ml_crps = crsp_monthly.copy(deep=True)
```

```{python}
# set the percentile to 20%
cut_off_quantile = 0.20
# create a series with the cut_off_quantile for each month
cut_off = ml_crps.groupby('month')['mktcap'].quantile(cut_off_quantile)
# Filter out the stocks with market capitalization below the 5th percentile
ml_crps = ml_crps[ml_crps['mktcap'] >= ml_crps['month'].map(cut_off)]
```

```{python}
# create a lagged excess return
r_df = pd.concat([ml_crps.groupby('permno')['ret_excess'].shift(i) for i in range(2,62)],axis=1) # new dataframe with r lagged
r_df.columns = [col+f'_lag_{i+2}' for i, col in enumerate(r_df.columns.values)]                  # rename columns
r_sq_df = r_df**2   # i
r_sq_df.columns = [col.replace('ret_excess', 'ret_excess_sq') for col in r_sq_df.columns.values]
ml_crps_lags = pd.concat([ml_crps, r_df, r_sq_df], axis=1)


# remove all nan values that arises from the lagging
ml_crps_lags.dropna(subset=[f'ret_excess_lag_{i}' for i in range(2, 62)],inplace=True)
ml_crps_lags.set_index(['permno', 'month'], inplace=True)
```

We then demean the dependent variable $r_{i,t+1}$ by month, subtracting the mean from the series, ensuring the mean excess return in each month is zero. Additionally, we standardize the lagged excess returns by month, dividing each by the standard deviation, making the standard deviation across firms one.

```{python}
# create the exogenous and endogenous variables
exog = ml_crps_lags.loc[:,[f'ret_excess_lag_{i}' for i in range(2,62)]+[f'ret_excess_sq_lag_{i}' for i in range(2,62)]]
endog = ml_crps_lags.loc[:,'ret_excess']

# Standardize the data - dependent variable has a mean of 0 and the predictors a standard deviation of 1
endog = endog - endog.groupby('month').mean()
exog = (exog-exog.groupby('month').mean()) / exog.groupby('month').std()

train_endog = endog.loc[endog.index.get_level_values('month') < '2000-01-01']
train_exog = exog.loc[exog.index.get_level_values('month') < '2000-01-01']
```

With the cleaned and transformed dataset, we proceed to choose the hyperparameter, $\lambda$, which minimizes the estimated mean squared prediction error (MSPE):
$$\lambda^{opt} = \arg\:\min_{\lambda} MSPE = E\left( \frac{1}{T} \sum^T_{t=1} (\hat{y}_t(\lambda) - y_t)^2 \right)$$

where $\hat{y}_t(\lambda)$ is the predicted value of the dependent variable at time $t$ for a given $\lambda$ and $y_t$ is the true value of the dependent variable at time $t$.

We divide the dataset into a training set for hyperparameter selection, using cross-validation to average the MSPE across K data folds over a range of hyperparameters. The hyperparameter that yields the lowest MSPE is then selected.

```{python}
initial_years = 5
assessment_months = 48*4
n_splits = 100
length_of_year = 12

data_folds = TimeSeriesSplit(
  n_splits=n_splits, 
  test_size=assessment_months, 
  max_train_size=initial_years * length_of_year
)
lambdas = np.logspace(-4, 4, 40) # evenly spaced on a logscale
```


We tune the model over a grid of $\lambda$ that is evenly spread on a logscale from `{python} lambdas.min().round(2)` to `{python} lambdas.max().round(2)` with 100 datafolds. 

```{python}

lambdas = np.insert(lambdas,0,0) #insert zero
params = {
  "alpha": lambdas,
}

# create pipeline
ridge_pipeline = Ridge(fit_intercept=False,max_iter=5000)

finder = GridSearchCV(
  estimator=ridge_pipeline,
  param_grid=params,
  scoring="neg_root_mean_squared_error", # RMSE -- https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter 
  cv=data_folds, # cross validation
  verbose=1, # Print messages choose 1 through 3 to get more information
  n_jobs=-1, # Use all cores
)

finder.fit(
  train_exog, train_exog
)

# store the best estimator
best_est = finder.best_estimator_

# store optimal penaly parameter
opt_lambda = best_est.get_params()['alpha']
```

```{python}
#| output: true
#| label: fig-tuning
#| fig-cap: Root mean squared error for different penalty

plot_df = pd.DataFrame({'RMSE':-finder.cv_results_['mean_test_score'],'lambda':lambdas})

opt_rmse = plot_df.loc[plot_df['lambda']==opt_lambda,'RMSE'].iloc[0]

plot_df['Sample'] = 'Full'
plot_df = pd.concat([plot_df,plot_df.loc[plot_df['RMSE']<0.682].assign(Sample='Zoom')])

(
  ggplot(plot_df, 
         aes(x="lambda", y="RMSE")) + 
  geom_line()  +
  facet_wrap("Sample",scales='free') +
  labs(x="$\lambda$", y="RMSE",) +
  scale_y_continuous() +
  theme(legend_position="none",figure_size=(12,6)) +
  scale_x_log10() +
  annotate("point", x = opt_lambda, y = opt_rmse, colour = "blue") +
  theme_minimal()
  ).draw()

```

After tuning, we set $\lambda$ to the optimal value (`{python} best_est.get_params()['alpha'].round(3)`) found, and estimate the model on the remaining dataset:

```{python}
# run the best estimator on the rest of the data
best_est_fit = best_est.fit(train_exog, train_endog)
coef = best_est_fit.coef_
```

We notice that the coefficient infront of $r_{i,t-k}^2$ seem to be large for $k = {12, 24, 36, 48}$. The corresponding lagged not squared excess return are negative. Meaning that the excess returns are predicted to be lower when the excess return same month in the previous years were posive. Based on this analysis, one might suggest to short those stocks that in a certain month previous years experienced positive excess returns.

In the @fig-coeffient_plot, we observe the coefficients for both the lagged excess returns ($r_{i,t−k}$) and their squared terms ($r^2_{i,t-k}$). It is noted that coefficient in front of squared terms seem to be greater in absolute value than those in front of $r_{i,t−k}$. 

The coefficients for the squared terms are all positive and relatively large up to the twelfth lag, after which they turn negative and mostly remain negative for the entire lag structure. Additionally, we notice significant coefficients for lags $k\in(12,24,48)$. This suggests that the excess returns exhibit some periodicity, where the squared returns from the same month in previous years strongly influence the current excess return.

The pattern for the coefficients of the non-squared lags of excess return is not as clear as for the squared terms. The coefficients are relatively small and, for lags up to a year, they are both positive and negative. There seems to be no evidence here for a momentum strategy. However, we do notice some clustering of high positive coefficients for lags of excess return between 1 to 2 years.

Based on this ridge-regression analysis, no horizon seem to justify a momentum strategy. 
```{python}
cut_func = lambda d: pd.qcut(np.abs(d),q=5,labels=['very low','low','medium','high','very high'])
plot_r_df = pd.DataFrame({'lag':range(2,len(coef)//2+2),'value':coef[len(coef)//2:]}).assign(model='r')
plot_r_df = plot_r_df.assign(color=cut_func(plot_r_df['value']))

plot_rsq_df = pd.DataFrame({'lag':range(2,len(coef)//2+2),'value':coef[:len(coef)//2]}).assign(model='r_sq')
plot_rsq_df = plot_rsq_df.assign(color=cut_func(plot_rsq_df['value']))
coef_df = pd.concat([plot_r_df, plot_rsq_df])

```

```{python}
#| output: true
#| label: fig-coeffient_plot
#| fig-cap: Coefficients obtained with tuned hyperparameter

(
  ggplot(coef_df, 
         aes(x="lag", y="value",fill='color')) + 
  geom_col()  +
  facet_wrap("model") +
  labs(x="Lag", y="Coefficient",) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none") + 
  theme_minimal() +
  scale_fill_manual(values=Blues.get_hex_swatch(3)+['#217dbf','#0a324f'])
  )

```

```{python}
#| output: true
#| label: fig-coeffient_scatter
#| fig-cap: Coefficients obtained with tuned hyperparameter

coef_df1 = coef_df.pivot(index='lag',columns='model',values='value')
beta = np.cov(coef_df1['r'].values, coef_df1['r_sq'].values) @ np.linalg.inv(np.var(coef_df1['r'].values))
# coef_df1['trend'] = coef_df1['r']*beta
# {i:1 for i in range(12,)}
(
  ggplot(coef_df1, 
         aes(x="r", y="r_sq")) + 
  geom_point()  +
  # geom_line(aes(coef_df1,x='r',y='trend'))+
  labs(x="r", y="r_sq",) +
  scale_x_continuous(labels=percent_format()) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none") + 
  theme_minimal()
  )

```