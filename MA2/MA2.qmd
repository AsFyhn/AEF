---
title: Mandatory Assignment 2
author: Asbjørn Fyhn & Emil Beckett Kolko
date: 2024-04-019
execute:
  echo: false
  warning: false
  output: false
format:
  pdf:
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
jupyter: python3
---

# Introduction

Jegadeesh and Titman (1993) documented that a portfolio that goes long high return stocks and short low return stocks over the past 3 to 12 months earns abnormal profits the following year. This phenomenon is known as the medium-term momentum effect. In this assignment, we first investigate this effect by estimating the returns of a long/short momentum portfolio using an ordinary least squares (OLS) regression. Next, we explore the predictions of cross-sectional stock returns using only past returns as predictors. This analysis is conducted through a machine learning framework that employs ridge regression. Instead of the positive correlation between last year’s returns and future returns found in Jegadeesh and Titman (1993), we find a negative correlation. Therefore, we conclude that a portfolio that goes long low return stocks and short high return stocks over the past 12 months earns abnormal profits.

# Exercise 1 and 2:

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm
import tabulate

from plotnine import *
from mizani.formatters import percent_format
from mizani.colors.brewer.sequential import Blues as Blues
from regtabletotext import prettify_result

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.model_selection import (
  train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score
)
import os
```

```{python}
#Connecting to the database
tidy_finance = sqlite3.connect(
    database=f"../../data/tidy_finance_python.sqlite"
)

#Reading in crsp_monthly and factors_ff3_monthly
crsp_monthly = (pd.read_sql_query(
    sql="SELECT * FROM crsp_monthly",
    con=tidy_finance,
    parse_dates={"month"})
)

factors_ff3_monthly = pd.read_sql_query(
  sql="SELECT month, mkt_excess FROM factors_ff3_monthly",
  con=tidy_finance,
  parse_dates={"month"}
)
```

We load the "tidy_finance_python.sqlite" database and select variables from the tables "crsp_monthly" and "factors_ff3_monthly". Thereafter, we compute the momentum of stock i as described in the assignment.

We do not expect significant differences between computing momentum as the relative change in prices compared to the relative change in market capitalization. In this assignment, we use the market capitalization as opposed to the stock price. The benefit of doing so is that our computations are not affected by events that increase or decrease the number of outstanding stocks, such as stock issuances or buybacks. Such events affect stock prices without affecting returns. However, when using the market capitalization, events such as dividend payouts, decrease market capitalization without affecting returns. Therefore, there are problems with both types of calculations. Lucklily, both the stock price and market capitalization has been adjusted accordingly in the CRSP dataset. Therefore, we should not face any problems by calculating momentum with either stocks prices or market capitalization.


```{python}
#Transforming the crsp_monthly dataset to contain mktcap_lag_12
mktcap_lag_12 = (crsp_monthly
  .assign(month=lambda x: x["month"]+pd.DateOffset(months=12))
  .get(["permno", "month", "mktcap"])
  .rename(columns={"mktcap": "mktcap_lag_12"})
  .dropna()
)

#Merging the datasets
data_for_sorts = (crsp_monthly
  .merge(mktcap_lag_12, how="inner", on=["permno", "month"])
)

#Computing the momentum of stock i for 12-month momentum
data_for_sorts['Mom_12'] = 100 * (data_for_sorts['mktcap_lag'] - data_for_sorts['mktcap_lag_12']) / data_for_sorts['mktcap_lag_12']
```

We present summary statistics of our dataset in @tbl-summary.



```{python}
#| output: true
#| label: tbl-summary
#| tbl-cap: Summary statistics of the dataset

# Summarizing the data for sorts with mean, std, min, 5%, 50% 95% and max
# List of variables to exclude
exclude_vars = ['month', 'permno']

# Filter out the excluded variables and displaying excess return in percentage terms
data_for_sorts_filtered = data_for_sorts.drop(columns=exclude_vars)

# Generate the summary statistics
data_for_sorts_summary = (
    data_for_sorts_filtered
    .describe(percentiles=[0.05, 0.5, 0.95])
    .round(2)
    .transpose()
)

data_for_sorts_summary
```
Where "ret_excess_p" is the monthly return above the risk free rate (0.01=1%), the "mktcap" variables are in USD millions, and "Mom_12" is the 12-month momentum in percentage terms (1=1%).

# Exercise 3:

```{python}
#| output: true
#Bullet 1
# Creating the decile groups for the 12-month momentum
quantile_func = lambda data: pd.qcut(data['Mom_12'], q=10, labels=False) + 1
quantiles = data_for_sorts.groupby('month').apply(quantile_func).reset_index().drop(columns='month').set_index('level_1').rename(columns={'Mom_12': 'Mom_12_decile'})
data_for_sorts = data_for_sorts.merge(quantiles, left_index=True, right_index=True)

# Merging the data with the factors_ff3_monthly dataset
data_for_sorts = data_for_sorts.merge(
    factors_ff3_monthly, how="inner", on="month"
)

#Sorting the data by decile and month
sorted_data = data_for_sorts.sort_values(
    by=['Mom_12_decile', 'month'])

#Bullet 2
#Calculating the equal-weighted average values of Mom_12 and mc for each of the ten portfolios
average_return = (
    sorted_data
    .groupby(['Mom_12_decile'])
    .apply(lambda x: pd.Series({
        'Average_Mom_12': np.mean(x['Mom_12']),
        'Average_mktcap': np.mean(x['mktcap'])
    }))
    .reset_index()
)
# Specifiying the headers for the table, which we will use later to create table 2.
headers = {'Mom_12_decile':"Decile",
    'Average_Mom_12':"Average mom",
    'Average_mktcap':"Average mktcap",
}
average_return.rename(columns=headers,inplace=True)

for col in list(headers.values()):
  average_return[col] = average_return[col].astype(float)
```

For each month, we divide the companies into 10 groups (deciles) of equal size based on their 12 month momentum. The first decile contains the companies with the lowest 12 month momentum and the tenth decile contains the companies with the highest 12 month momentum. 

We report the average market cap and 12-month momentum for each decile portfolio in @tbl-alphasacross. Here, we equal-weight the weights of the stocks in the portfolios.  
Unsurprisingly, the table shows that the average market momentum increases with the momentum decile, but also that the average market capitalization is increasing in the momentum decile until the 7th decile.\
Based on a value-weighting, we calculate the average excess return for the ten momentum-sorted portfolios which is reported alongside the portfolio characteristics in @tbl-alphasacross. 

Using the value-weighted portfolios, we estimate alpha of each portfolio with OLS utilizing the CAPM equation: 
$$r_{i,t}-r_{f,t}=\alpha_i+\beta_i(r_{m,t}-r_{f,t})+\epsilon_{i,t}$$
We report the alphas in @fig-capmalphas with their corresponding confidence intervals in the @fig-capmalphas. 

```{python}
# Computing the value weighted excess returns for each of the ten portfolios
Vw_excess_returns = (sorted_data.groupby(["Mom_12_decile", "month"])
  .apply(lambda x: x.assign(
      ret=np.average(x["ret_excess"], weights=x["mktcap_lag"])
    )
  )).reset_index(drop=True)
```

```{python}
#| output: true
# Calculating the CAPM alpha for each of the ten portfolios
# Defining the linear regression function
def lin_reg(x,y):
    """
    Function that runs a linear regression of y on x
    and returns the alpha, t-statistic, p-value and standard error
        Args:
            x (array): independent variable
            y (array): dependent variable
        Returns:
            alpha (float): alpha coefficient
            t-statistic (float): t-statistic of the alpha coefficient
            p-value (float): p-value of the alpha coefficient
            std_err (float): standard error of the alpha coefficient
    """
    reg = sm.OLS(y, sm.add_constant(x)).fit()
    return reg.params["const"], reg.tvalues['const'], reg.pvalues['const'], reg.bse['const']

# Running the CAPM regression for each of the ten portfolios
CAPM_alphas = (
    Vw_excess_returns.groupby(["Mom_12_decile"])
    .apply(
        lambda x: lin_reg(x["mkt_excess"], x["ret"])
    )
    .reset_index()
)
# unfolding the tuples into columns
CAPM_alphas['Alpha'] = [x[0] for x in CAPM_alphas[0]]
CAPM_alphas['t-statistic'] = [x[1] for x in CAPM_alphas[0]]
CAPM_alphas['p-value'] = [x[2] for x in CAPM_alphas[0]]
CAPM_alphas['std_err'] = [x[3] for x in CAPM_alphas[0]]
CAPM_alphas.drop(columns=[0], inplace=True)


# Merging the calculations to one dataset to present it in a table
Alphas_excessret = pd.merge(
  Vw_excess_returns.groupby("Mom_12_decile")['ret'].mean().reset_index(),
  CAPM_alphas, 
  how="left", 
  on="Mom_12_decile"
)

# Specifiying the headers for the table
headers_v2 = {'Mom_12_decile':"Decile",
    'ret':"Excess Return",
}
# Renaming the headers
Alphas_excessret.rename(columns=headers_v2,inplace=True)

# Putting headers in place
for col in list(headers_v2.values()):
  Alphas_excessret[col] = Alphas_excessret[col].astype(float)
```

```{python}
#| output: true
#| label: tbl-alphasacross
#| tbl-cap: Alphas across the ten portfolios

# merge 
merged = pd.merge(
  average_return,
  Alphas_excessret.drop(columns='std_err'),
  on='Decile',
  how='outer'
  )
  
# Specifying decimals before printing the table
formatted_dfAgg_v2 = merged.style.format({
    'Decile': '{:.0f}',
    'Average mom': '{:,.2f}',
    'Average mktcap' : '{:,.0f}',
    'Excess ret': '{:.4f}',
    'Alpha': '{:.4f}',
    't-statistic': '{:.2f}',
    'p-value': '{:.2f}',
    'na_rep': "" 
})
# hide index
formatted_dfAgg_v2.hide()
formatted_dfAgg_v2
```



```{python}
#| output: true
#| label: fig-capmalphas
#| fig-cap: CAPM alphas of momentum-sorted portfolios

# Calculate the 5% con
Alphas_excessret['CI_lower'] = (Alphas_excessret['Alpha'] - 1.96 * Alphas_excessret['std_err'])
Alphas_excessret['CI_upper'] = (Alphas_excessret['Alpha'] + 1.96 * Alphas_excessret['std_err'])
# Plotting the alphas on the momentum portfolios
plot_momentum_portfolios_summary = (
  ggplot(Alphas_excessret, 
         aes(x="Decile", y="Alpha", fill="Decile")) +
  geom_bar(stat="identity") +
  geom_errorbar( 
      aes(x='Decile', ymin='CI_lower', ymax='CI_upper'), 
      width=0.4, colour="orange", alpha=0.9, size=1.3) + 
  labs(x="Decile", y="CAPM alpha") +
  scale_x_continuous(breaks=range(1, 11)) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
    theme(legend_position="none",
          figure_size=(6, 4))
)
plot_momentum_portfolios_summary.draw()
```

From @fig-capmalphas, we see that the lowest decile generates the highest alpha eventhough we expected them to have negative alphas in accordance with Jegadeesh and Titman (1993). One possible explanation for this, is that stocks that pays out dividends, will decrease their market cap (and therefore have low momentum) while maintaining returns. However, the CRSP dataset should have adjusted for this, and therefore we will not investigate this notion further. 
The rest of the momentum decile stocks are more aligned with our expectations and the findings in Jegadeesh and Titman (1993). The 2nd and 3rd decile stocks have negative alphas, and the stocks in the 5th decile and up offer positive alpha, which represents an imperfection in the market. 

In the following, we test if we can exploit this imperfection with a momentum strategy that goes long past winners and short past losers.
Specifically, we examine the alpha and beta of a portfolio that shorts the 1st decile portfolio and goes long the 10th decile portfolio. We note that this entails shorting stocks with high alpha to buy stocks with lower alpha, which seems counter-intuitive. We then estimate the portfolio's alpha and beta with OLS of the following regression model:
$$r_{p,t} = \alpha + \beta r_{m,i}$$
where $r_{p,t}$ is the return of the portfolio, and $r_{m,i}$ is the return of the market index. The alpha and beta are estimated with the Newey-West standard errors with a lag length of 12 months. The results of the estimation are presented in @tbl-performance:


```{python}
# Naming 1 decile portfolio as low and 10 decile portfolio as high
Vw_excess_returns['portfolio'] = np.where(Vw_excess_returns["Mom_12_decile"]==1,'low',np.where(Vw_excess_returns["Mom_12_decile"]==10,'high','neutral'))

# Pivoting the data to get the long-short momentum portfolio
mom_ls = (Vw_excess_returns
          .query("portfolio in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio", values="ret")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )
# Running the CAPM regression for the long-short momentum portfolio
model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1 + mkt_excess", 
    data=mom_ls
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)
```

```{python}
#| output: true
#| label: tbl-performance
#| tbl-cap: Performance of the momentum strategy
# Format the DataFrame style
output = (pd.DataFrame(index=['alpha','beta'],
                        data=({
                            'Estimate':model_fit.params.values, # parameters
                            'Std. Error': model_fit.bse.values,  # standard errors
                            't-Value':model_fit.tvalues.values, # t-values
                            'p-Value':model_fit.pvalues.values, # p-values
                            })))
output
```

We cannot reject the null hypothesis that the return, $\alpha$, of the momentum strategy is 0. This finding is in accordance with the CAPM model, which states that arbitrage strategies are not possible. However, our results deviate from Jegadeesh and Titman (1993) who suggest that it is possible to obtain a positive $\alpha$ with the momentum strategy. The deviation in our results can be explained by the high alphas of portfolio 1, which are not usually observed.

Our results show a significant negative $\beta$, which means the momentum (long/short) portfolio is negatively correlated with the market. This type of portfolio can be used to reduce the market risk of other investments with positive betas.

Next, we calculate the excess return of the (long/short) portfolio described above, and for a portfolio (high) that only goes long in the stocks of the 10th decile and a portfolio (low) that exclusively goes long in the stocks of the 1st decile. \
The yearly returns for the three portfolios are plotted in @fig-annualreturns
```{python}
#| output: true
#| label: fig-annualreturns
#| fig-cap: Annual returns of momentum portfolios

# Calculate annual returns for the 'low', 'high', and 'long_short' portfolios and reshape data for plotting
momentum_longshort_year = (mom_ls
  .assign(year=lambda x: x["month"].dt.year)
  .groupby("year")
  .aggregate(
    low=("low", lambda x: (1+x).prod()),
    high=("high", lambda x: (1+x).prod()),
    long_short=("long_short", lambda x: (1+x).prod())
  )
  .reset_index()
  .melt(id_vars="year", var_name="name", value_name="value")
)
# Create a bar plot of annual returns
plot_momentum_longshort_year = (
  ggplot(momentum_longshort_year, 
         aes(x="year", y="value", fill="name")) +
  geom_col(position='dodge') +
  facet_wrap("~name", ncol=1,scales='free') +
  labs(x="", y="") +
  scale_color_discrete(guide=False) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
  theme(legend_position="none",
          figure_size=(6, 4)))
plot_momentum_longshort_year.draw()
```

The graphs generally show an expected pattern with both positive and negative returns. All portfolios exhibit autocorrelation in the returns, which supports the argument for a momentum portfolio. Interestingly, the momentum long/short portfolio delivers higher returns more consistently than both the low and high momentum portfolio which have positive alphas. 

# Exercise 4

We aim to estimate a model where a stock's excess return is linearly dependent on its own 60 previous lags. The regression model is as follows:

$$r_{i,t+1} = \sum_{k=1}^{60} b_k r_{i,t-k} + \sum_{k=1}^{60} c_k r_{i,t-k}^2$$

The squared term represents the magnitude of past returns, irrespective of their direction (positive or negative).Therefore, the coefficient $c_k$ captures how much the volatility (or magnitude of returns) from k periods ago affects the current period's excess return.

To estimate the coefficients of this model, we will use ridge regression, a regularized version of OLS regression. Ridge regression helps reduce the variance of the estimates by adding a penalty term to the OLS regression.This penalty term is the sum of the squared coefficients multiplied by a constant, $\lambda$, a hyperparameter that must be chosen. The optimal value of $\lambda$ is found through cross-validation. The regression coefficient has a closed-form solution:

$$\hat{\beta} = (X^TX + \lambda I)^{-1}X^Ty$$

Here $\hat{\beta}$ represents the estimated coefficients, $X$ is the matrix of predictors, $y$ is the vector of the dependent variable, and $I$ is the identity matrix.

First, we clean the dataset by removing the smallest stocks, specifically those with the lowest 20% market capitalization.

```{python}
# slice so we only use stocks that are listed on NYSE
ml_crps = crsp_monthly.loc[crsp_monthly['exchange']=='NYSE']
```

```{python}
# set the percentile to 20%
cut_off_quantile = 0.20
# create a series with the cut_off_quantile for each month
cut_off = ml_crps.groupby('month')['mktcap'].quantile(cut_off_quantile)
# Filter out the stocks with market capitalization below the 5th percentile
ml_crps = ml_crps[ml_crps['mktcap'] >= ml_crps['month'].map(cut_off)]
```

```{python}
# Generate lagged columns efficiently
lags = range(2, 62)
lagged_data = {f'ret_lag_{i}': ml_crps.groupby('permno')['ret'].shift(i) for i in lags}

# Create DataFrame from lagged columns
r_df = pd.DataFrame(data = lagged_data)

# Generate squared lagged columns
squared_lagged_data = {f'ret_sq_lag_{i}': r_df[f'ret_lag_{i}'] ** 2 for i in lags}

# Create DataFrame from squared lagged columns
r_sq_df = pd.DataFrame(squared_lagged_data)

# # Concatenate original DataFrame with lagged and squared lagged DataFrames
ml_crps_lags = pd.concat([ml_crps,r_df, r_sq_df], axis=1) # 

# Efficiently remove rows with NaN values in any of the lagged columns
mask = ml_crps_lags.notna().all(axis=1)
ml_crps_lags = ml_crps_lags[mask.values]

# Set the index
ml_crps_lags.sort_values(['permno', 'month'], inplace=True)
ml_crps_lags.set_index(['permno', 'month'], inplace=True)
```

We then demean the dependent variable $r_{i,t+1}$ by month, subtracting the mean from the series, ensuring the mean excess return in each month is zero. Additionally, we standardize the lagged excess returns by month, dividing each by the standard deviation, making the standard deviation across firms one.

```{python}
lag_cols = list(lagged_data.keys())
squared_lag_cols = list(lagged_data.keys())

# create the exogenous and endogenous variables
exog = ml_crps_lags[lag_cols+squared_lag_cols]
endog = ml_crps_lags.get('ret')

# Standardize the data - dependent variable has a mean of 0 and the predictors a standard deviation of 1
endog = endog - endog.groupby('month').mean()
exog = (exog-exog.groupby('month').mean()) / exog.groupby('month').std()

train_endog = endog.loc[endog.index.get_level_values('month') < '2000-01-01']
train_exog = exog.loc[exog.index.get_level_values('month') < '2000-01-01']
```

With the cleaned and transformed dataset, we proceed to choose the hyperparameter, $\lambda$, which minimizes the estimated mean squared prediction error (MSPE):
$$\lambda^{opt} = \arg\:\min_{\lambda} MSPE = E\left( \frac{1}{T} \sum^T_{t=1} (\hat{y}_t(\lambda) - y_t)^2 \right)$$

where $\hat{y}_t(\lambda)$ is the predicted value of the dependent variable at time $t$ for a given $\lambda$ and $y_t$ is the true value of the dependent variable at time $t$.

We divide the dataset into a training set for hyperparameter selection, using cross-validation to average the MSPE across K data folds over a range of hyperparameters. The hyperparameter that yields the lowest MSPE is then selected.

```{python}
initial_years = 5
assessment_months = 48*4
n_splits = 100
length_of_year = 12

data_folds = TimeSeriesSplit(
  n_splits=n_splits, 
  test_size=assessment_months, 
  max_train_size=initial_years * length_of_year
)
lambdas = np.logspace(-4, 4, 40) # evenly spaced on a logscale
```


We tune the model over a grid of $\lambda$ that is evenly spread on a logscale from `{python} lambdas.min().round(2)` to `{python} lambdas.max().round(2)` with 100 datafolds. 

```{python}

lambdas = np.insert(lambdas,0,0) #insert zero
params = {
  "alpha": lambdas,
}

# create pipeline
ridge_pipeline = Ridge(fit_intercept=False,max_iter=5000)

finder = GridSearchCV(
  estimator=ridge_pipeline,
  param_grid=params,
  scoring="neg_root_mean_squared_error", # RMSE -- https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter 
  cv=data_folds, # cross validation
  verbose=1, # Print messages choose 1 through 3 to get more information
  n_jobs=-1, # Use all cores
)

finder.fit(
  train_exog, 
  train_endog
)

# store the best estimator
best_est = finder.best_estimator_

# store optimal penaly parameter
opt_lambda = best_est.get_params()['alpha']
```

```{python}
#| output: true
#| label: fig-tuning
#| fig-cap: Root mean squared error for different penalty

plot_df = pd.DataFrame({'RMSE':-finder.cv_results_['mean_test_score'],'lambda':lambdas})

opt_rmse = plot_df.loc[plot_df['lambda']==opt_lambda,'RMSE'].iloc[0]

plot_df['Sample'] = 'Full'
plot_df = pd.concat([plot_df,plot_df.loc[plot_df['RMSE']<0.682].assign(Sample='Zoom')])

(
  ggplot(plot_df, 
         aes(x="lambda", y="RMSE")) + 
  geom_line()  +
  facet_wrap("Sample",scales='free') +
  labs(x="$\lambda$", y="RMSE",) +
  scale_y_continuous() +
  theme(legend_position="none",figure_size=(12,6)) +
  scale_x_log10() +
  annotate("point", x = opt_lambda, y = opt_rmse, colour = "blue") +
  theme_minimal()
  ).draw()

```

After tuning, we set $\lambda$ to the optimal value (`{python} best_est.get_params()['alpha'].round(3)`) found, and estimate the model on the remaining dataset:

```{python}
# run the best estimator on the rest of the data
best_est_fit = best_est.fit(train_exog, train_endog)
coef = best_est_fit.coef_
```

We notice that the coefficient infront of $r_{i,t-k}^2$ seem to be large for $k = {12, 24, 36, 48}$. The corresponding lagged not squared excess return are negative. Meaning that the excess returns are predicted to be lower when the excess return same month in the previous years were posive. Based on this analysis, one might suggest to short those stocks that in a certain month previous years experienced positive excess returns.

In the @fig-coeffient_plot, we observe the coefficients for both the lagged excess returns ($r_{i,t−k}$) and their squared terms ($r^2_{i,t-k}$). It is noted that coefficient in front of squared terms seem to be greater in absolute value than those in front of $r_{i,t−k}$. 

The coefficients for the squared terms are all positive and relatively large up to the twelfth lag, after which they turn negative and mostly remain negative for the entire lag structure. Additionally, we notice significant coefficients for lags $k\in(12,24,48)$. This suggests that the excess returns exhibit some periodicity, where the squared returns from the same month in previous years strongly influence the current excess return.

The pattern for the coefficients of the non-squared lags of excess return is not as clear as for the squared terms. The coefficients are relatively small and, for lags up to a year, they are both positive and negative. There seems to be no evidence here for a momentum strategy. However, we do notice some clustering of high positive coefficients for lags of excess return between 1 to 2 years.

Based on this ridge-regression analysis, no horizon seem to justify a momentum strategy. 
```{python}
cut_func = lambda d: pd.qcut(np.abs(d),q=5,labels=['very low','low','medium','high','very high'])
plot_r_df = pd.DataFrame({'lag':range(2,len(coef)//2+2),'value':coef[len(coef)//2:]}).assign(model='r')
plot_r_df = plot_r_df.assign(color=cut_func(plot_r_df['value']))

plot_rsq_df = pd.DataFrame({'lag':range(2,len(coef)//2+2),'value':coef[:len(coef)//2]}).assign(model='r_sq')
plot_rsq_df = plot_rsq_df.assign(color=cut_func(plot_rsq_df['value']))
coef_df = pd.concat([plot_r_df, plot_rsq_df])

```

```{python}
#| output: true
#| label: fig-coeffient_plot
#| fig-cap: Coefficients obtained with tuned hyperparameter

(
  ggplot(coef_df, 
         aes(x="lag", y="value",fill='color')) + 
  geom_col()  +
  facet_wrap("model") +
  labs(x="Lag", y="Coefficient",) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none") + 
  theme_minimal() +
  scale_fill_manual(values=Blues.get_hex_swatch(3)+['#217dbf','#0a324f'])
  )

```
