---
title: Mandatory Assignment 2
author: "Exam number: 73 & 85"
date: 2024-06-10
execute:
  echo: false
  warning: false
  output: false
format:
  pdf:
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
jupyter: python3
---

# Introduction

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm

from plotnine import *
from mizani.formatters import percent_format
from mizani.colors.brewer.sequential import Blues as Blues

from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit

#Connecting to the database
tidy_finance = sqlite3.connect(
    database=f"../../data/tidy_finance_python.sqlite"
)
```

Jegadeesh and Titman (1993) documented that a portfolio that goes long high return stocks and short low return stocks over the past 3 to 12 months earns abnormal profits the following year. This phenomenon is known as the medium-term momentum effect. In this assignment, we first investigate this effect by estimating the returns of a long/short momentum portfolio using an ordinary least squares (OLS) regression. Next, we explore the predictions of cross-sectional stock returns using only past returns as predictors. This analysis is conducted through a machine learning framework that employs ridge regression. We find a positive correlation between last yearâ€™s returns and future returns as in Jegadeesh and Titman (1993). A portfolio that goes long high momentum stocks and short low momentum stocks over the past 6-12 months earns abnormal returns. However, 2-5 months and 13-20 months lagged momentum long/short portfolios earn negative excess returns.

# Exercise 1 and 2:

```{python}
#Reading in crsp_monthly and factors_ff3_monthly
crsp_monthly = (pd.read_sql_query(
    sql="SELECT permno, month, ret, ret_excess, mktcap, mktcap_lag, exchange FROM crsp_monthly",
    con=tidy_finance,
    parse_dates={"month"})
)

factors_ff3_monthly = pd.read_sql_query(
  sql="SELECT month, mkt_excess FROM factors_ff3_monthly order by month",
  con=tidy_finance,
  parse_dates={"month"}
)

#Transforming the crsp_monthly dataset to contain mktcap_lag_12
mktcap_lag_12 = (crsp_monthly
  .assign(month=lambda x: x["month"]+pd.DateOffset(months=12))
  .get(["permno", "month", "mktcap"])
  .rename(columns={"mktcap": "mktcap_lag_12"})
  .dropna()
)

#Merging the datasets
data_for_sorts = (crsp_monthly
  .merge(mktcap_lag_12, how="inner", on=["permno", "month"])
)

#Computing the momentum of stock i for 12-month momentum
data_for_sorts['Mom_12'] = 100 * (data_for_sorts['mktcap_lag'] - data_for_sorts['mktcap_lag_12']) / data_for_sorts['mktcap_lag_12']
```

We load the "tidy_finance_python.sqlite" database and select variables from the tables "crsp_monthly" and "factors_ff3_monthly". Thereafter, we compute the momentum of stock i as described in the assignment.

We do not expect significant differences between computing momentum as the relative change in prices compared to the relative change in market capitalization. In this assignment, we use the market capitalization as opposed to the stock price. The benefit of doing so is that our computations are not affected by events that increase or decrease the number of outstanding stocks, such as stock issuances or buybacks. Such events affect stock prices without affecting returns. However, when using the market capitalization, events such as dividend payouts, decrease market capitalization without affecting returns. Therefore, there are problems with both types of calculations. Lucklily, both the stock price and market capitalization has been adjusted accordingly in the CRSP dataset. Therefore, we should not face any problems by calculating momentum with either stocks prices or market capitalization.


We present summary statistics of our dataset in @tbl-summary. The mean of 12-Month momentum is around $20\%$ but the distribution has long tails. Likewise, we see that market cap has a long right tail of its distribution. 

```{python}
#| output: true
#| label: tbl-summary
#| tbl-cap: Summary statistics of the dataset

# Generate the summary statistics: count, mean, std, min, 50%, and max
(
    data_for_sorts[['ret','ret_excess', 'Mom_12', 'mktcap']]
    .assign(
        ret=lambda x: x['ret']*100,
        ret_excess=lambda x: x['ret_excess']*100,
    )
    .rename({'ret': 'Return', 'ret_excess': 'Excess Return', 'mktcap': 'Market Cap', 'Mom_12': '12-Month Momentum'}, axis=1)
    .describe(percentiles=[])
    .transpose()
    .map(lambda x: "{:,.2f}".format(x)) # Format the numbers with 2 decimal places
    .assign(
        count=lambda x: x['count'].apply(lambda val: val.replace(".00", "")), # Remove the decimal places for count
        max=lambda x: x['max'].apply(lambda val: val.split(".")[0]), # Remove the decimal places for max
    )
    .rename(columns={'count': 'Count', 'mean': 'Mean', 'std': 'Std', 'min': 'Min', '50%': 'Median', 'max': 'Max'})
)

```

# Exercise 3:

```{python}
#| output: true
#Bullet 1
# Creating the decile groups for the 12-month momentum
quantile_func = lambda data: pd.qcut(data['Mom_12'], q=10, labels=False) + 1
quantiles = data_for_sorts.groupby('month').apply(quantile_func).reset_index().drop(columns='month').set_index('level_1').rename(columns={'Mom_12': 'Mom_12_decile'})

sorted_data = (data_for_sorts
                  .merge(quantiles, left_index=True, right_index=True) # Merge with the deciles
                  .merge(factors_ff3_monthly, how="left", on="month") # Merge with the factors
                  .sort_values(by=['Mom_12_decile', 'month']) # Sort by decile and month
                  )
```

In this exercise we analyze the performance of a momentum strategy. This is done by examning the performance of portfolios created based past returns. Specifally, we compute monthly portfolio breakpoints using 12-month momentum ($mom^{12}_{i,t}$) as the sorting variable for each month $t$. Subsequently, we divide the companies into ten deciles based on their 12-month momentum. Specifically, the first decile encompasses companies with the lowest 12-month momentum, while the tenth decile comprises those with the highest 12-month momentum. This method ensures that each month's portfolio breakpoints accurately reflect the momentum distribution across all stocks in the sample, facilitating effective portfolio stratification and analysis.

To create portfolios, we start by equal-weighting the stocks in each decile. The average market cap and 12-month momentum for each decile portfolio is reported in @tbl-alphasacross. Unsurprisingly, the table shows that the average market momentum increases with the momentum decile, but also that the average market capitalization is increasing in the momentum decile until the 7th decile.

Now, instead of creating equal-weighted portfolio, we create a value-weighted portfolio. The weights are then determined by a stock's relative size to the other stocks within the decile. The analysis continues with the value_weighted portfolios. For each decile-portfolio, we calculate the average excess return for the ten decile which is reported alongside the portfolio characteristics in @tbl-alphasacross.

```{python}
# Computing the value weighted excess returns for each of the ten portfolios
Vw_excess_returns = pd.merge(
    sorted_data.drop(columns=['ret']), # Dropping the return column
    sorted_data.groupby(["Mom_12_decile", "month"]).apply(
        lambda x: np.average(x["ret_excess"], weights=x["mktcap_lag"])
    ).rename("ret"),
    left_on=["Mom_12_decile", "month"],
    right_index=True,
    how="left",
    )

### Less efficient way of computing the value weighted excess returns
# Vw_excess_returns = (sorted_data.groupby(["Mom_12_decile", "month"])
#   .apply(lambda x: x.assign(
#       ret=np.average(x["ret_excess"], weights=x["mktcap_lag"])
#     )
#   )).reset_index(drop=True)

##### Calculating the CAPM alpha for each of the ten portfolios
def lin_reg(x,y):
    """
    Function that runs a linear regression of y on x
    and returns the alpha, t-statistic, p-value and standard error
        Args:
            x (array): independent variable
            y (array): dependent variable
        Returns:
            alpha (float): alpha coefficient
            t-statistic (float): t-statistic of the alpha coefficient
            p-value (float): p-value of the alpha coefficient
            std_err (float): standard error of the alpha coefficient
    """
    reg = sm.OLS(y, sm.add_constant(x)).fit()
    return reg.params["const"], reg.tvalues['const'], reg.pvalues['const'], reg.bse['const']

# Running the CAPM regression for each of the ten portfolios
CAPM_alphas = (
    Vw_excess_returns
    .groupby("Mom_12_decile")
    .apply(
        lambda x: pd.Series(lin_reg(x["mkt_excess"], x["ret"]),index=["Alpha", "t-statistic", "p-value", "std_err"])
    )
    .reset_index()
    )

```


```{python}
#| output: true
#| label: tbl-alphasacross
#| tbl-cap: Average momentum and excess return

# Merge to include excess return in table
merged = pd.merge(
  # Equal weighted average of momentum and market cap
  (sorted_data
    .groupby(['Mom_12_decile'])
    .apply(lambda x: pd.Series({
        'Average_Mom_12': np.mean(x['Mom_12']),
        'Average_mktcap': np.mean(x['mktcap'])
    }))
    .reset_index()), 
  # Value weighted average of excess return
  (Vw_excess_returns
   .groupby("Mom_12_decile")['ret']
   .mean()
   .reset_index()),
  on='Mom_12_decile',
  how='outer' # anything goes
  )
(
    merged
    .assign(
        Mom_12_decile=lambda x: x['Mom_12_decile'].map(lambda val: f"{val:.0f}"),
        ret = lambda x: x['ret'].map(lambda val: f"{val:,.4f}"),
        Average_Mom_12 = lambda x: x['Average_Mom_12'].map(lambda val: f"{val:,.2f}"),
        Average_mktcap = lambda x: x['Average_mktcap'].map(lambda val: f"{val:,.0f}"),
        )
    .rename(columns={
        'Mom_12_decile':'Decile',
        'Average_Mom_12': 'Average momentum',
        'Average_mktcap': 'Average market cap',
        'ret':'Excess Return'}, 
        )
    .set_index('Decile')
)


```

Using the value-weighted portfolios, we estimate alpha of each portfolio with OLS utilizing the CAPM equation: 
$$r_{i,t}-r_{f,t}=\alpha_i+\beta_i(r_{m,t}-r_{f,t})+\epsilon_{i,t}$$
We report the alphas and their corresponding 95% confidence intervals in @fig-capmalphas. 

```{python}
#| output: true
#| label: fig-capmalphas
#| fig-cap: CAPM alphas of momentum-sorted portfolios

# Calculate the 5% con
CAPM_alphas['CI_lower'] = (CAPM_alphas['Alpha'] - 1.96 * CAPM_alphas['std_err'])
CAPM_alphas['CI_upper'] = (CAPM_alphas['Alpha'] + 1.96 * CAPM_alphas['std_err'])
# Plotting the alphas on the momentum portfolios
plot_momentum_portfolios_summary = (
  ggplot(CAPM_alphas.rename(columns={'Mom_12_decile': 'Decile'}), 
         aes(x="Decile", y="Alpha", fill="Decile")) +
  geom_bar(stat="identity") +
  geom_errorbar( 
      aes(x='Decile', ymin='CI_lower', ymax='CI_upper'), 
      width=0.4, colour="orange", alpha=0.9, size=1.3) + 
  labs(x="Decile", y="CAPM alpha") +
  scale_x_continuous(breaks=range(1, 11)) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
    theme(legend_position="none",
          figure_size=(6, 4))
)
plot_momentum_portfolios_summary.draw()
```

From @fig-capmalphas, we see that the lowest decile generates the lowest alpha and generally a higher momentum means higher alpha. This result is in accordance with Jegadeesh and Titman (1993) and represents an imperfection in the market. In the following, we test if we can exploit this imperfection with a momentum strategy that goes long past winners and short past losers. Specifically, we estimate the alpha and beta of a portfolio that shorts the 1st decile portfolio and goes long the 10th decile portfolio with OLS of the following regression model:
$$r_{p,t} = \alpha + \beta r_{m,t}+\epsilon_t$$
where $r_{p,t}$ is the excess return of the long/short portfolio, and $r_{m,t}$ is the excess return of the market index. To adjust for autocorrelation we use the Newey and West t-statistic with a lag length of 12 months. The results of the estimation are presented in @tbl-performance.


```{python}
#| output: true
#| label: tbl-performance
#| tbl-cap: Performance of the momentum strategy

# Naming 1 decile portfolio as low and 10 decile portfolio as high
Vw_excess_returns['portfolio'] = Vw_excess_returns["Mom_12_decile"].map({1: "low", 10: "high"})

# Pivoting the data to get the long-short momentum portfolio
mom_ls = (Vw_excess_returns
          .query("portfolio in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio", values="ret")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )
# Running the CAPM regression for the long-short momentum portfolio
model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1 + mkt_excess", 
    data=mom_ls
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)

# Format the DataFrame style
output = (pd.DataFrame(index=['alpha','beta'],
                        data=({
                            'Estimate':model_fit.params.values, # parameters
                            'Std. Error': model_fit.bse.values,  # standard errors
                            't-Value':model_fit.tvalues.values, # t-values
                            'p-Value':model_fit.pvalues.values, # p-values
                            })))
output.round(3)
```

We reject the null hypothesis that the return, $\alpha$, of the momentum strategy is 0. This result aligns with the result in Jegadeesh and Titman (1993) that suggest it is possible to obtain a positive $\alpha$ with the momentum strategy. However, our results deviate from the CAPM, which states that arbitrage strategies are not possible. Furthermore, our results show a significant negative $\beta$, which means the momentum (long/short) portfolio is negatively correlated with the market. This type of portfolio can be used to reduce the market risk of other investments with positive betas.


Next, we calculate the excess return of the (long/short) portfolio described above, and for a portfolio (high) that only goes long in the stocks of the 10th decile and a portfolio (low) that exclusively goes long in the stocks of the 1st decile. The yearly returns for the three portfolios are plotted in @fig-annualreturns


```{python}
#| output: true
#| label: fig-annualreturns
#| fig-cap: Annual returns of momentum portfolios

# Calculate annual returns for the 'low', 'high', and 'long_short' portfolios and reshape data for plotting
momentum_longshort_year = (mom_ls
  .assign(year=lambda x: x["month"].dt.year)
  .groupby("year")
  .aggregate(
    low=("low", lambda x: (1+x).prod()-1),
    high=("high", lambda x: (1+x).prod()-1),
    long_short=("long_short", lambda x: (1+x).prod()-1)
  )
  .reset_index()
  .melt(id_vars="year", var_name="name", value_name="value")
)
# Create a bar plot of annual returns
plot_momentum_longshort_year = (
  ggplot(momentum_longshort_year, 
         aes(x="year", y="value", fill="name")) +
  geom_col(position='dodge') +
  facet_wrap("~name", ncol=1,scales='free') +
  labs(x="", y="") +
  scale_color_discrete(guide=False) +
  scale_y_continuous(labels=percent_format()) +
  theme_minimal() +
  theme(legend_position="none",
          figure_size=(7, 4)))
plot_momentum_longshort_year.draw()
```

The graphs generally show an expected pattern with both positive and negative returns. All portfolios exhibit autocorrelation in the returns, which supports the argument for a momentum portfolio. Interestingly, the momentum (long/short) portfolio performs poorly in 2009, the year after the financial crisis. This is due to the stellar performance of low momentum stocks bouncing back from the low point in 2008.

# Exercise 4

```{python}
# slice so we only use stocks that are listed on NYSE
ml_crps = crsp_monthly.loc[crsp_monthly['exchange']=='NYSE',['permno','month','ret_excess','mktcap']]

# set the percentile to 20%
cut_off_quantile = 0.20
# create a series with the cut_off_quantile for each month
cut_off = ml_crps.groupby('month')['mktcap'].quantile(cut_off_quantile)
# Filter out the stocks with market capitalization below the 5th percentile
ml_crps = ml_crps[ml_crps['mktcap'] >= ml_crps['month'].map(cut_off)]

# Generate lagged columns efficiently
lags = range(2, 62)
lagged_data = {f'ret_lag_{i}': ml_crps.groupby('permno')['ret_excess'].shift(i) for i in lags}

# Create DataFrame from lagged columns
r_df = pd.DataFrame(data = lagged_data)

# Generate squared lagged columns
squared_lagged_data = {f'ret_sq_lag_{i}': r_df[f'ret_lag_{i}'] ** 2 for i in lags}

# Create DataFrame from squared lagged columns
r_sq_df = pd.DataFrame(squared_lagged_data)

# # Concatenate original DataFrame with lagged and squared lagged DataFrames
ml_crps_lags = pd.concat([ml_crps,r_df, r_sq_df], axis=1) # 

# Efficiently remove rows with NaN values in any of the lagged columns
mask = ml_crps_lags.notna().all(axis=1)
ml_crps_lags = ml_crps_lags[mask.values]

# Set the index
ml_crps_lags.sort_values(['month','permno'], inplace=True)
ml_crps_lags.set_index(['permno', 'month'], inplace=True)

lag_cols = list(lagged_data.keys())
squared_lag_cols = list(squared_lagged_data.keys())

# create the exogenous and endogenous variables
exog = ml_crps_lags[lag_cols+squared_lag_cols]
endog = ml_crps_lags.get('ret_excess')

# Standardize the data - dependent variable has a mean of 0 and the predictors a standard deviation of 1
endog = endog - endog.groupby('month').mean()
exog = (exog-exog.groupby('month').mean()) / exog.groupby('month').std()

# create a grid of 40 values between 10^-4 and 10^4
lambdas = np.logspace(-4, 4, 40) 
params = {
  "alpha": lambdas,
}
# set TimeSeriesSplit parameters
datafolds = 10 # Number of splits/folds for cross-validation
test_size = 4*12 # Set the size of the test set
max_train_size = 5*12 # Set the maximum size of the training set

data_folds = TimeSeriesSplit(
  n_splits=datafolds, 
  test_size= test_size, 
  max_train_size= max_train_size 
)


# create pipeline
ridge_pipeline = Ridge(
  fit_intercept=False, # no intercept due to demeaning of variables
  max_iter=5000 # max iterations
  )

finder = GridSearchCV(
  estimator=ridge_pipeline,
  param_grid=params,
  scoring="neg_root_mean_squared_error", # RMSE -- https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter 
  cv=data_folds, # cross validation
  verbose=1, # Print messages choose 1 through 3 to get more information
  n_jobs=-1, # Use all cores
)

# fit the model
finder.fit(
  X=exog, 
  y=endog
)

# store the best estimator
best_est = finder.best_estimator_

# store optimal penaly parameter
opt_lambda = finder.best_params_['alpha']
opt_score = -finder.best_score_

```

We now investigate how stocks' excess return at a given time is influenced by its own past returns and the magnitudes of those returns. We do so by estimation the following model:
$$r_{i,t} = \sum_{k=2}^{61} b_k r_{i,t-k} + \sum_{k=2}^{61} c_k r_{i,t-k}^2$$

The coefficients, $b_k$ represent the linear relationship between the excess return at time $t$ and the excess return from $k$ periods ago. As such, the positive $b_k$ suggests that higher returns $k$ periods ago are associated with higher returns at time $t$.
The squared term represents the magnitude of past returns, irrespective of their direction (positive or negative). Therefore, the coefficient $c_k$ captures how much the volatility from $k$ periods ago affects the current period's excess return.

We will estimate the parameters using the CRSP dataset. We clean the dataset by removing the smallest stocks, specifically those with the lowest 20% market capitalization. Further, we demean the dependent variable $r_{i,t}$ by month, subtracting the mean from the series, ensuring the mean excess return in each month is zero. Additionally, we demean and standardize all lagged returns by month, dividing each by the standard deviation, making the mean equal to zero and the standard deviation across firms one.


To estimate the coefficients of this model, we will use ridge regression, a regularized version of OLS regression. Ridge regression helps reduce the variance of the estimates by adding a penalty term to the OLS regression. This penalty term is the sum of the squared coefficients multiplied by a constant, $\lambda$, a hyperparameter that must be chosen. The optimal value of $\lambda$ is found through cross-validation. The regression coefficient has a closed-form solution:

$$\hat{\beta} = (X^TX + \lambda I)^{-1}X^TY$$

Here $\hat{\beta}$ represents the estimated coefficients, $X$ is the matrix of predictors, $Y$ is the vector of the dependent variable, $I$ is the identity matrix, and $\lambda$ is a hyperparameter. The value of $\lambda$ will be chosen to minimize the estimated mean squared prediction error (MSPE):
$$\lambda^{opt} = \arg\:\min_{\lambda} MSPE = E\left( \frac{1}{T} \sum^T_{t=1} (\hat{r}_{i,t}(\lambda) - r_{i,t})^2 \right)$$

where $\hat{y}_t(\lambda)$ is the predicted value of the dependent variable at time $t$ for a given $\lambda$ and $y_t$ is the true value of the dependent variable at time $t$.
To select the optimal hyperparameter $\lambda$, we employ a suitable cross-validation scheme. Specifically, we divide the dataset into $K$ folds and perform cross-validation, calculating the MSPE for each fold over a range of $\lambda$ values. The cross-validation procedure averages the MSPE across all $K$ folds for each $\lambda$. The hyperparameter that results in the lowest average MSPE is then selected as the optimal $Î»$. This approach ensures that the selected 
$\lambda$ generalizes well to unseen data, balancing model complexity and predictive accuracy.
We divide the dataset into a training set for hyperparameter selection, using cross-validation to average the MSPE across $K$ data folds over a range of hyperparameters. The hyperparameter that yields the lowest MSPE is then selected. Choosing $K$ involves balancing the computational burden with the desire for accurate estimates of model performance. 

We tune the model over a grid of $\lambda$ that is evenly spread on a logscale from `{python} lambdas.min().round(2)` to `{python} lambdas.max().round(2)` with `{python} datafolds` datafolds. We present the average RMSE across the datafolds for the different values of the hyperparameters in the grid in @fig-tuning. The tuning of the model yields $\lambda=$`{python} opt_lambda.round(2)`. We thus proceed and estimate the model on the remaining dataset:

```{python}
#| output: true
#| label: fig-tuning
#| fig-cap: Root mean squared error for different penalty

plot_df = pd.DataFrame({'RMSE':-finder.cv_results_['mean_test_score'],'lambda':lambdas})
plot_df['Sample'] = 'Full'

index = plot_df.loc[plot_df['lambda']==opt_lambda].index[0]
plot_df = pd.concat([plot_df,plot_df.iloc[index-3:index+4].assign(Sample='Zoom')])

(
  ggplot(plot_df, 
         aes(x="lambda", y="RMSE")) + 
  geom_line()  +
  facet_wrap("Sample",scales='free') +
  labs(x="$\lambda$", y="RMSE",) +
  scale_y_continuous() +
  theme(legend_position="none",figure_size=(6,3)) +
  scale_x_log10() +
  annotate("point", x = opt_lambda, y = opt_score, colour = "blue") +
  theme_minimal()
  ).draw()
```

In the @fig-coeffientplot, we observe the coefficients for both the lagged excess returns ($r_{i,tâˆ’k}$) and their squared terms ($r^2_{i,t-k}$). \
We notice a distinct pattern in the estimated coefficients:

- For $k<6$, the coefficient, $\hat{b}_k$, is negative, indicating a negative relationship between past excess returns and future returns. The $\hat{c_k}$ are relatively large. Indicating a short term relationship between volatility (large move in either direction) and excess return in the current period. Any pattern in $\hat{c_k}$ is vanished.

- However, from $6 < k \leq 12$. For $k>12$, there is a shift, with six consecutive positive coefficients, suggesting a reversal in the trend towards positive relationships.

- Beyond $12<k$, the majority of coefficients are negative, albeit with a few exceptions. Notably, all $\hat{b}_k$ values are positive for $k\in\{12,24,36,48,60\}$, hinting at a periodicity in the data.

Interpreting these coefficient estimates, we find indications of mean reversion effects in the short run, particularly for periods under 6 months, with the effect peaking at $k=2$ and diminishing thereafter. \
In the medium run (6-12 months), there's evidence suggesting a momentum strategy. Positive excess returns experienced 6-12 months ago seem to influence the stock's performance in period $t$.

```{python}
#| output: true
#| label: fig-coeffientplot
#| fig-cap: Estimated coefficients

# run the best estimator on the rest of the data
names = best_est.feature_names_in_
coefficients = best_est.coef_

# Create a dataframe with the coefficients
a = pd.DataFrame({'feature':names, 'value':coefficients})

# get the last int from feature name
a['lag'] = a['feature'].str.extract('(\d+)').astype(int)

# get which coefficient 
a['coeff'] = np.where(a['feature'].str.contains('ret_sq'), 'r_sq', 'r')
a['coeff'] = a['coeff'].map({'r_sq':'$\hat{c}_k$','r':'$\hat{b}_k$'})
# determine the color of the coefficient based on the absolute value
cut_func = lambda d: pd.qcut(np.abs(d),q=5,labels=['very low','low','medium','high','very high'])
a['Abs. Value'] = a.groupby('coeff')['value'].apply(cut_func).values

# plot the coefficients \hat{b_k} and \hat{}
(
  ggplot(a, 
         aes(x="lag", y="value",fill='Abs. Value')) + 
  geom_col()  +
  facet_wrap("coeff",) +
  labs(x=f"$k$ (lags)", y="Coefficient estimate") +
  scale_y_continuous() +
  theme(legend_position="none") + 
  theme_minimal() +
  scale_fill_manual(values=Blues.get_hex_swatch(3)+['#217dbf','#0a324f'])
  )
```


