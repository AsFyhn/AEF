---
title: "Mandatory Assignment 2"
author: "Asbj√∏rn Fyhn & Emil Beckett Kolko"
date: "2024-04-019"
execute: 
  echo: false
  warning: false
  output: false
jupyter: python3
format:
  pdf: 
    number-sections: true
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
---

**Exercise 1 and 2:**


```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm
import tabulate

from plotnine import *
from mizani.formatters import percent_format
from regtabletotext import prettify_result

#Connecting to the database
tidy_finance = sqlite3.connect(database="/Users/emilkolko/Downloads/tidy_finance_python.sqlite")

#Reading in crsp_monthly and factors_ff3_monthly
crsp_monthly = (pd.read_sql_query(
    sql="SELECT permno, month, ret_excess, mktcap, mktcap_lag FROM crsp_monthly",
    con=tidy_finance,
    parse_dates={"month"})
)

factors_ff3_monthly = pd.read_sql_query(
  sql="SELECT month, mkt_excess FROM factors_ff3_monthly",
  con=tidy_finance,
  parse_dates={"month"}
)
```

When computing the momentum we use the market capitalization as opposed to the stock price. The benefit of doing so, is that our computations are not affected by events that increase or decrease the number of outstanding stocks such as stock issuances or buybacks. If we had used the stock price in our computations as opposed to the market capitalization, such events would have artificially increased or decreased our calculated momentum.
```{python}
#Transforming the crsp_monthly dataset to contain mktcap_lag_12
mktcap_lag_12 = (crsp_monthly
  .assign(month=lambda x: x["month"]+pd.DateOffset(months=12))
  .get(["permno", "month", "mktcap"])
  .rename(columns={"mktcap": "mktcap_lag_12"})
  .dropna()
)

#Merging the datasets
data_for_sorts = (crsp_monthly
  .merge(mktcap_lag_12, how="inner", on=["permno", "month"])
)

#Computing the momentum of stock i for 12-month momentum
data_for_sorts['Mom_12'] = 100 * (data_for_sorts['mktcap_lag'] - data_for_sorts['mktcap_lag_12']) / data_for_sorts['mktcap_lag_12']

#Printing the first 24 rows of the dataset to check if the data transformation was successful
print(data_for_sorts.head(24))
```

**Exercise 3:**

The equal-weighted average values of the 12-month momentum (Average_Mom_12) and market capitalization (Average_mktcap) is shown in the table 1 below:
```{python}
# | output : true
#Bullet 1
#Creating the decile groups for the 12-month momentum

data_for_sorts['Mom_12_decile'] = pd.qcut(data_for_sorts['Mom_12'], q=10, labels=False) + 1

#Sorting the data by decile and month
sorted_data = data_for_sorts.sort_values(
    by=['Mom_12_decile', 'month'])

#Bullet 2
#Calculating the equal-weighted average values of Mom_12 and mc for each of the ten portfolios
average_return = (
    sorted_data
    .groupby(['Mom_12_decile'])
    .apply(lambda x: pd.Series({
        'Average_Mom_12': np.mean(x['Mom_12']),
        'Average_mktcap': np.mean(x['mktcap'])
    }))
    .reset_index()
)

# Specifiying the headers for the table
headers = {'Mom_12_decile':"Momentum decile",
    'Average_Mom_12':"Average momentum",
    'Average_mktcap':"Average market cap",
}
average_return.rename(columns=headers,inplace=True)
# # Printing the table
# print(tabulate.tabulate(average_return, headers, tablefmt="simple", showindex=False))

for col in list(headers.values()):
  average_return[col] = average_return[col].astype(float)

# Format the DataFrame style
formatted_dfAgg = average_return.style.format({
    'Momentum decile': '{:.0f}',  # No decimals
    'Average momentum': '{:.2f}',  # Two decimal places
    'Average market cap': '{:.2f}',  # Two decimal places
    'na_rep': "" 
})
formatted_dfAgg.hide()
formatted_dfAgg
```

Furthermore, we present the average excess return and the CAPM alpha for the ten momentum-sorted portfolios in the table 2. The excess return is reported in the "factors_ff3_monthly" dataset and the CAPM alphas are estimated with OLS from the CAPM equation: 

$r_{i,t}-r_{f,t}=\alpha_i+\beta_i(r_{m,t}-r_{f,t})+\epsilon_{i,t}$ 


The null-hypothosis is that alpha is zero and the alternative hypothosis is that alpha is different from zero:

$H_0: \alpha_i=0$ 

$H_A: \alpha_i\neq 0$

```{python}
# | output : true
# Bullet 3
# Computing the value weighted excess returns for each of the ten portfolios
Vw_excess_returns = (
    sorted_data.groupby(["Mom_12_decile"])
    .apply(
        lambda x: pd.Series(
            {
                "Decile_portfolio_excess_return": np.average(x["ret_excess"], weights=x["mktcap"])
            }
        )
    )
    .reset_index()
)

# Calculating the CAPM alpha for each of the ten portfolios
# Merging the data with the factors_ff3_monthly dataset
data_for_sorts = data_for_sorts.merge(
    factors_ff3_monthly, how="inner", on="month"
)
# Defining the linear regression function
def lin_reg(x,y):
    reg = sm.OLS(y, sm.add_constant(x)).fit()
    return reg.params["const"], reg.tvalues['const'], reg.pvalues['const']

# Running the CAPM regression for each of the ten portfolios
CAPM_alphas = (
    data_for_sorts.groupby(["Mom_12_decile"])
    .apply(
        lambda x: lin_reg(x["mkt_excess"], x["ret_excess"])
    )
    .reset_index()
)
CAPM_alphas['Alpha'] = [x[0] for x in CAPM_alphas[0]]
CAPM_alphas['t-statistic'] = [x[1] for x in CAPM_alphas[0]]
CAPM_alphas['p-value'] = [x[2] for x in CAPM_alphas[0]]
CAPM_alphas.drop(columns=[0], inplace=True)


# Merging the calculations to one dataset to present it in a table
Alphas_excessret = Vw_excess_returns.merge(
    CAPM_alphas, how="left", on="Mom_12_decile"
)

# Specifiying the headers for the table
headers_v2 = {'Mom_12_decile':"Momentum decile",
    'Decile_portfolio_excess_return':"Excess return",
}
# Renaming the headers
Alphas_excessret.rename(columns=headers_v2,inplace=True)

# Putting headers in place
for col in list(headers_v2.values()):
  Alphas_excessret[col] = Alphas_excessret[col].astype(float)

# Format the DataFrame style
formatted_dfAgg_v2 = Alphas_excessret.style.format({
    'Momentum decile': '{:.0f}',  # No decimals
    'Excess return': '{:.4f}', # 4 decimal places
    'Alpha': '{:.4f}',
    't-statistic': '{:.2f}',
    'p-value': '{:.2f}',
    'na_rep': "" 
})
formatted_dfAgg_v2.hide()
formatted_dfAgg_v2
```
We see from the t-statistics and p-values, that we can reject the null-hypothosis in all cases except for the portfolio of 4th decile momentum stocks. As such, alpha is significantly different from zero in all other cases. To get a better sense of the distribution of alpha accross the ten portfolios, we present the data in a graph:
```{python}
# | output : true
# Plotting the alphas on the momentum portfolios
plot_momentum_portfolios_summary = (
  ggplot(Alphas_excessret, 
         aes(x="Momentum decile", y="Alpha", fill="Momentum decile")) +
  geom_bar(stat="identity") +
  labs(x="Momentum decile", y="CAPM alpha", fill="Momentum decile",
       title="CAPM alphas of momentum-sorted portfolios") +
  scale_x_continuous(breaks=range(1, 11)) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none")
)
plot_momentum_portfolios_summary.draw()
```

We see that the lowest decile stocks offer high alphas compared to the rest of the stocks. We expected the lowest performing stocks to have a negative alpha in accordance with most of the literature. The 2nd and 3rd decile stocks are more aligned with the literature showing negative alphas and the stocks in the 5th decile and up offer positive alphas. These findings represent an imperfection in the market. In the following, we test if we can exploit this imperfection with a momentum strategy that goes long past winners and short past losers. We note that this entails shorting stocks with high alpha to buy stocks with lower alpha, which must yield a negative return.

Specifically, we examine the returns of a portfolio that shorts the 1st decile portfolio and goes long the 10th decile portfolio. We analyze the returns of this portfolio with t-statistics. Here our hypothesizes are:

$H_0: \alpha=0$ 

$H_A: \alpha\neq 0$

To compute the Newey-West standard errors we must choose a bandwidth based on lags for the estimation. The choice here is rather arbitrary and not data-driven, but we choose a lag length of 12 months, since that is also the lag length of the momentum stocks. Our results are presented below:



```{python}
# | output : true
data_for_sorts['portfolio'] = np.where(data_for_sorts["Mom_12_decile"]==1,'low',np.where(data_for_sorts["Mom_12_decile"]==10,'high','neutral'))

mom_ls = (data_for_sorts
          .query("portfolio in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio", values="ret_excess")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )

model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1 + mkt_excess", 
    data=mom_ls
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)
prettify_result(model_fit)
```

We cannot reject the null hypothesis that the returns of the momentum strategy is 0. This finding is in accordance with the CAPM model, that arbitrage strategies are not possible. However, our results deviate from most literature which suggests that it is possible to obtain a positive return with the momentum strategy. The deviation in our results can be explained by the high alphas of portfolio 1, which is not usually observed.

Perhaps we could have could have argued that the stocks in the 1st decile are outliers and cut them from our dataset. When we perform the above exercise on portfolio 2 and 10, we obtain the below results:

```{python}
# | output : true
data_for_sorts['portfolio_opt'] = np.where(data_for_sorts["Mom_12_decile"] == 2, 'low', np.where(data_for_sorts["Mom_12_decile"] == 10, 'high', 'neutral'))

mom_ls_opt = (data_for_sorts
          .query("portfolio_opt in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio_opt", values="ret_excess")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )

model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1", 
    data=mom_ls_opt
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)
prettify_result(model_fit)
```
Now the momentum strategy yields a positive return in accordance with the literature. However, this type of data cherry picking is problematic, and we will not pursue the approach further.

