---
title: "Mandatory Assignment 2"
author: "Asbj√∏rn Fyhn & Emil Beckett Kolko"
date: "2024-04-019"
execute: 
  echo: false
  warning: false
  output: false
jupyter: python3
format:
  pdf: 
    number-sections: true
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
---

**Exercise 1 and 2:**

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm
import tabulate

from plotnine import *
from mizani.formatters import percent_format
from regtabletotext import prettify_result

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.model_selection import (
  train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score
)

#Connecting to the database
tidy_finance = sqlite3.connect(
    database=f"/Users/asbjornfyhn/Desktop/Emp Fin/data/tidy_finance_python.sqlite"
)

#Reading in crsp_monthly and factors_ff3_monthly
crsp_monthly = (pd.read_sql_query(
    sql="SELECT permno, month, ret_excess, mktcap, mktcap_lag FROM crsp_monthly",
    con=tidy_finance,
    parse_dates={"month"})
)

factors_ff3_monthly = pd.read_sql_query(
  sql="SELECT month, mkt_excess FROM factors_ff3_monthly",
  con=tidy_finance,
  parse_dates={"month"}
)
```

When computing the momentum we use the market capitalization as opposed to the stock price. The benefit of doing so, is that our computations are not affected by events that increase or decrease the number of outstanding stocks such as stock issuances or buybacks. If we had used the stock price in our computations as opposed to the market capitalization, such events would have artificially increased or decreased our calculated momentum.
```{python}
#Transforming the crsp_monthly dataset to contain mktcap_lag_12
mktcap_lag_12 = (crsp_monthly
  .assign(month=lambda x: x["month"]+pd.DateOffset(months=12))
  .get(["permno", "month", "mktcap"])
  .rename(columns={"mktcap": "mktcap_lag_12"})
  .dropna()
)

#Merging the datasets
data_for_sorts = (crsp_monthly
  .merge(mktcap_lag_12, how="inner", on=["permno", "month"])
)

#Computing the momentum of stock i for 12-month momentum
data_for_sorts['Mom_12'] = 100 * (data_for_sorts['mktcap_lag'] - data_for_sorts['mktcap_lag_12']) / data_for_sorts['mktcap_lag_12']

#Printing the first 24 rows of the dataset to check if the data transformation was successful
print(data_for_sorts.head(24))
```

**Exercise 3:**

The equal-weighted average values of the 12-month momentum (Average_Mom_12) and market capitalization (Average_mktcap) is shown in the table 1 below:
```{python}
# | output : true
#Bullet 1
#Creating the decile groups for the 12-month momentum

data_for_sorts['Mom_12_decile'] = pd.qcut(data_for_sorts['Mom_12'], q=10, labels=False) + 1

#Sorting the data by decile and month
sorted_data = data_for_sorts.sort_values(
    by=['Mom_12_decile', 'month'])

#Bullet 2
#Calculating the equal-weighted average values of Mom_12 and mc for each of the ten portfolios
average_return = (
    sorted_data
    .groupby(['Mom_12_decile'])
    .apply(lambda x: pd.Series({
        'Average_Mom_12': np.mean(x['Mom_12']),
        'Average_mktcap': np.mean(x['mktcap'])
    }))
    .reset_index()
)

# Specifiying the headers for the table
headers = {'Mom_12_decile':"Momentum decile",
    'Average_Mom_12':"Average momentum",
    'Average_mktcap':"Average market cap",
}
average_return.rename(columns=headers,inplace=True)
# # Printing the table
# print(tabulate.tabulate(average_return, headers, tablefmt="simple", showindex=False))

for col in list(headers.values()):
  average_return[col] = average_return[col].astype(float)

# Format the DataFrame style
formatted_dfAgg = average_return.style.format({
    'Momentum decile': '{:.0f}',  # No decimals
    'Average momentum': '{:.2f}',  # Two decimal places
    'Average market cap': '{:.2f}',  # Two decimal places
    'na_rep': "" 
})
formatted_dfAgg.hide()
formatted_dfAgg
```

Furthermore, we present the average excess return and the CAPM alpha for the ten momentum-sorted portfolios in table 2. The excess returns are reported in the "factors_ff3_monthly" dataset and the CAPM alphas are estimated with OLS from the CAPM equation: 

$r_{i,t}-r_{f,t}=\alpha_i+\beta_i(r_{m,t}-r_{f,t})+\epsilon_{i,t}$ 


The null-hypothosis is that alpha is zero and the alternative hypothosis is that alpha is different from zero:

$H_0: \alpha_i=0$ 

$H_A: \alpha_i\neq 0$

```{python}
# | output : true
# Bullet 3
# Computing the value weighted excess returns for each of the ten portfolios
Vw_excess_returns = (
    sorted_data.groupby(["Mom_12_decile"])
    .apply(
        lambda x: pd.Series(
            {
                "Decile_portfolio_excess_return": np.average(x["ret_excess"], weights=x["mktcap"])
            }
        )
    )
    .reset_index()
)

# Calculating the CAPM alpha for each of the ten portfolios
# Merging the data with the factors_ff3_monthly dataset
data_for_sorts = data_for_sorts.merge(
    factors_ff3_monthly, how="inner", on="month"
)
# Defining the linear regression function
def lin_reg(x,y):
    reg = sm.OLS(y, sm.add_constant(x)).fit()
    return reg.params["const"], reg.tvalues['const'], reg.pvalues['const']

# Running the CAPM regression for each of the ten portfolios
CAPM_alphas = (
    data_for_sorts.groupby(["Mom_12_decile"])
    .apply(
        lambda x: lin_reg(x["mkt_excess"], x["ret_excess"])
    )
    .reset_index()
)
CAPM_alphas['Alpha'] = [x[0] for x in CAPM_alphas[0]]
CAPM_alphas['t-statistic'] = [x[1] for x in CAPM_alphas[0]]
CAPM_alphas['p-value'] = [x[2] for x in CAPM_alphas[0]]
CAPM_alphas.drop(columns=[0], inplace=True)


# Merging the calculations to one dataset to present it in a table
Alphas_excessret = Vw_excess_returns.merge(
    CAPM_alphas, how="left", on="Mom_12_decile"
)

# Specifiying the headers for the table
headers_v2 = {'Mom_12_decile':"Momentum decile",
    'Decile_portfolio_excess_return':"Excess return",
}
# Renaming the headers
Alphas_excessret.rename(columns=headers_v2,inplace=True)

# Putting headers in place
for col in list(headers_v2.values()):
  Alphas_excessret[col] = Alphas_excessret[col].astype(float)

# Format the DataFrame style
formatted_dfAgg_v2 = Alphas_excessret.style.format({
    'Momentum decile': '{:.0f}',  # No decimals
    'Excess return': '{:.4f}', # 4 decimal places
    'Alpha': '{:.4f}',
    't-statistic': '{:.2f}',
    'p-value': '{:.2f}',
    'na_rep': "" 
})
formatted_dfAgg_v2.hide()
formatted_dfAgg_v2
```

We see from the t-statistics and p-values, that we can reject the null-hypothosis in all cases except for the portfolio of 4th decile momentum stocks. As such, alpha is significantly different from zero in all other cases. To get a better sense of the distribution of alpha accross the ten portfolios, we present the data in a graph:
```{python}
# | output : true
# Plotting the alphas on the momentum portfolios
plot_momentum_portfolios_summary = (
  ggplot(Alphas_excessret, 
         aes(x="Momentum decile", y="Alpha", fill="Momentum decile")) +
  geom_bar(stat="identity") +
  labs(x="Momentum decile", y="CAPM alpha", fill="Momentum decile",
       title="CAPM alphas of momentum-sorted portfolios") +
  scale_x_continuous(breaks=range(1, 11)) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none")
)
plot_momentum_portfolios_summary.draw()
```

We see that the lowest decile stocks offer high alphas compared to the rest of the stocks. We expected the lowest performing stocks to have a negative alpha in accordance with most of the literature. The 2nd and 3rd decile stocks are more aligned with the literature showing negative alphas and the stocks in the 5th decile and up offer positive alphas. These findings represent an imperfection in the market. In the following, we test if we can exploit this imperfection with a momentum strategy that goes long past winners and short past losers.

Specifically, we examine the alpha and beta of a portfolio that shorts the 1st decile portfolio and goes long the 10th decile portfolio. We note that this entails shorting stocks with high alpha to buy stocks with lower alpha, which seems counter-intuitive. We analyze the alpha and beta of this portfolio with t-statistics. Here our hypothesizes are:

$H_0: \alpha=0$ 

$H_A: \beta=0$

To compute the Newey-West standard errors we must choose a bandwidth based on lags for the estimation. The choice here is rather arbitrary and not data-driven, but we choose a lag length of 12 months, since that is also the lag length of the momentum stocks. Our results are presented below:



```{python}
# | output : true
data_for_sorts['portfolio'] = np.where(data_for_sorts["Mom_12_decile"]==1,'low',np.where(data_for_sorts["Mom_12_decile"]==10,'high','neutral'))

mom_ls = (data_for_sorts
          .query("portfolio in ['low', 'high']")
          .pivot_table(index="month", columns="portfolio", values="ret_excess")
          .assign(long_short=lambda x: x["high"]-x["low"])
          .merge(factors_ff3_monthly, how="left", on="month")
  )

model_fit = (sm.OLS.from_formula(
    formula="long_short ~ 1 + mkt_excess", 
    data=mom_ls
  )
  .fit(cov_type="HAC", cov_kwds={"maxlags": 12})
)
prettify_result(model_fit)
```

We cannot reject the null hypothesis that the returns, $\alpha$, of the momentum strategy is 0. This finding is in accordance with the CAPM model, that arbitrage strategies are not possible. However, our results deviate from most literature which suggests that it is possible to obtain a positive $\alpha$ with the momentum strategy. The deviation in our results can be explained by the high alphas of portfolio 1, which is not usually observed.

Our results show a significant negative $\beta$, which means the momentum (long/short) portfolio is negatively correlated with the market. This type of portfolio is rare and can be used to reduce the market risk of other investments with positive betas. This type of return mimics the return of gold, which historically have a negative $\beta$.

To further our understanding of the returns of the portfolio we plot the graph below:

```{python}
# | output : true
momentum_longshort_year = (mom_ls
  .assign(year=lambda x: x["month"].dt.year)
  .groupby("year")
  .aggregate(
    low=("low", lambda x: 1-(1+x).prod()),
    high=("high", lambda x: 1-(1+x).prod()),
    long_short=("long_short", lambda x: 1-(1+x).prod())
  )
  .reset_index()
  .melt(id_vars="year", var_name="name", value_name="value")
)

plot_momentum_longshort_year = (
  ggplot(momentum_longshort_year, 
         aes(x="year", y="value", fill="name")) +
  geom_col(position='dodge') +
  facet_wrap("~name", ncol=1) +
  labs(x="", y="", title="Annual returns of momentum portfolios") +
  scale_color_discrete(guide=False) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none")
)
plot_momentum_longshort_year.draw()
```

The graphs generally show an expected pattern with both positive and negative returns. All portfolios exhibit autocorrelation in the returns, which further the argument for a momentum portfolio. Interestingly, the momentum long/short portfolio seems to deliver the highest returns even though the two other portfolios have positive alphas. 

**Exercise 4** 
In this exercise, we want to estimate a model where the excess return of a stock is linear dependent on its own 60 lags. The regression model looks as follows: 

$r_{i,t+1} 0 \sum_{k=1}^{60} b_k r_{i,t-k} + \sum_{k=1}^{60} c_k r_{i,t-k}^2$

First, we start by cleaning the dataset meaning that we remove the smallest stocks. More precisely, we remove the stocks with the 5% lowest market capitalization. 

```{python}

# remove all companies that has a mktcap that is below the 20th percentile
for month in crsp_monthly["month"].unique():
    mktcap_20 = crsp_monthly[crsp_monthly["month"] == month]["mktcap"].quantile(0.05)
    crsp_monthly = crsp_monthly.loc[~((crsp_monthly['month'] == month) & (crsp_monthly['mktcap'] < mktcap_20))]

# create 60 columns with the lagged ret_excess
crsp_monthly_lags = crsp_monthly.copy(deep=True)
for i in range(2, 62):
    crsp_monthly_lags[f'ret_excess_lag_{i}'] = crsp_monthly_lags.groupby('permno')['ret_excess'].shift(i)
    crsp_monthly_lags[f'ret_excess_lag_{i}_sq'] = crsp_monthly_lags[f'ret_excess_lag_{i}']**2

crsp_monthly_lags.dropna(subset=[f'ret_excess_lag_{i}' for i in range(2, 62)],inplace=True)
crsp_monthly_lags.set_index(['permno', 'month'], inplace=True)
exog = crsp_monthly_lags[[f'ret_excess_lag_{i}' for i in range(2,62)]+[f'ret_excess_lag_{i}_sq' for i in range(2,62)]]
endog = crsp_monthly_lags.loc[:,'ret_excess']

```

Next, we demean the dependent variable $r_{i,t+1}$ by month by month subtracting the mean from series. Thus by construction, the mean of the excess return in each month is 0. At the same time, we standardize the lagged excess return by month by month dividing with the standard deviation. In each month, the standard deviation across the firms is one. 

```{python}
# Standardize the data - dependent variable has a mean of 0 and the predictors a standard deviation of 1
endog = endog - endog.groupby('month').mean()
exog = exog / exog.groupby('month').std()

train_endog = endog.loc[endog.index.get_level_values('month') < '2000-01-01']
train_exog = exog.loc[exog.index.get_level_values('month') < '2000-01-01']
```

The entire dataset has now been cleaning and transformed in such a way that we are ready to take the analysis further. We will estimate regression coefficient using ridge regression. Thus, we will have to choose the hyperparameter, $\alpha$. 

The hyperparameter is chosen by minimizing the estimated mean squared prediction error, MSPE.

$MSPE = E\left( \frac{1}{T} \sum^T_{t=1} (\hat{y}_t - y_t)^2 \right)$

Before fine tuning the model, we divide our dataset into a training set for hyperparameter selection. Since MSPE can only be estimated, we employ cross-validation. This involves averaging the MSPE across K data folds over a range of hyperparameters. The hyperparameter that yields the lowest MSPE is then selected.

```{python}
initial_years = 5
assessment_months = 48*4
n_splits = 100
length_of_year = 12
alphas = np.logspace(-4, 4, 40)

data_folds = TimeSeriesSplit(
  n_splits=n_splits, 
  test_size=assessment_months, 
  max_train_size=initial_years * length_of_year
)
```

We tune the model over a grid of alphas that goes from `{python} alphas.min().round(2)` to `{python} alphas.max().round(2)` with 100 datafolds. 

```{python}
params = {
  "alpha": alphas,
}
ridge_pipeline = Ridge(fit_intercept=False,max_iter=5000)

finder = GridSearchCV(
  estimator=ridge_pipeline,
  param_grid=params,
  scoring="neg_root_mean_squared_error", # RMSE -- https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter 
  cv=data_folds, # cross validation
  verbose=1, # Print messages choose 1 through 3 to get more information
  n_jobs=-1, # Use all cores
)

finder.fit(
  train_exog, train_exog
)

best_est = finder.best_estimator_
best_est_fit = best_est.fit(train_exog, train_endog)
coef = best_est_fit.coef_

```

After tuning the model, we set alpha to `{python} best_est.get_params()['alpha'].round(3)`

```{python}
# | output : true
heatmap = pd.DataFrame({'lag':range(2,len(coef)//2+2),'r':coef[len(coef)//2:],'r^2':coef[:-len(coef)//2]})
( ggplot(heatmap) +
  geom_col(aes(x='lag', y='r'), position='dodge', fill="blue", alpha=0.5) +  # Bar chart for 'r'
  geom_point(aes(x='lag', y='r^2'), color="red", size=3) +  # Scatter plot for 'r^2'
  labs(x="lag", y="Coefficient", title="") +
  scale_color_discrete(guide=False) +
  scale_y_continuous(labels=percent_format()) +
  theme(legend_position="none")
).draw()
```

We notice that the coefficient infront of $r_{i,t-k}^2$ seem to be large for $k = {12, 24, 36, 48}$. The corresponding lagged not squared excess return are negative. Meaning that the excess returns are predicted to be lower when the excess return same month in the previous years were posive. Based on this analysis, one might suggest to short those stocks that in a certain month previous years experienced positive excess returns.

\newpage

```{python}
# | output : true
heatmap = pd.concat([heatmap.iloc[(i-1)*len(heatmap)//3:i*len(heatmap)//3].reset_index(drop=True) for i in range(1,4)],axis=1)
heatmap.columns = ['lag','r','r^2','lag ','r ','r^2 ','lag  ','r  ','r^2  ']
heatmapTable = heatmap.style.background_gradient(cmap='RdYlBu',subset=['lag','r','r^2','lag ','r ','r^2 ','lag  ','r  ','r^2  '])
heatmapTable.hide()
```

