---
title: Mandatory Assignment 2
author: Exam number 73 & 85
date: 2024-06-10
execute:
  echo: false
  warning: false
  output: false
format:
  pdf:
    number-sections: true
    colorlinks: true
    geometry:
      - top=20mm
      - left=20mm
      - bottom=20mm
      - right=20mm
    cite-method: natbib
    fontsize: 12pt
jupyter: python3
---
```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.api as sm
import tabulate

from copy import deepcopy
from plotnine import *
from mizani.formatters import percent_format
from mizani.colors.brewer.sequential import Blues as Blues
from regtabletotext import prettify_result
import os
from scipy.optimize import minimize

```


```{python}
#Load data
df = pd.read_csv(r"../../data/data-exam-2024.csv") 
# slice data set into training and test
df_train = df.loc[df['month']<='2015-12-31']
df_test = df.loc[df['month']>'2015-12-31']

# Number of stocks and months in the training dataset
N = df_train['permno'].nunique()
T = df_train['month'].nunique()
gamma = 4
```

**Exercise 1**
```{python}
df_ff = deepcopy(df_train)
def ff_regression(data: pd.DataFrame,residuals:bool=True):
    """This function runs the regression of the ff model. 
        Args: 
          data (pd.DataFrame): data used for the regression
          residuals (bool): determines whether to return residuals or parameters"""
    y_value = data['ret_excess']
    x_values = data[['mkt_excess','smb','hml']]
    res = sm.OLS(y_value, sm.add_constant(x_values)).fit(cov_type='HC1')

    if residuals:
        return_values = res.resid
    else: 
        return_values = res.params
    return return_values

# run the regression for each permno
ff_reg_coef_unstack = df_ff.groupby('permno').apply(ff_regression,residuals=False)
ff_reg_coef_unstack = ff_reg_coef_unstack.rename(columns={'const':'alpha','mkt_excess':'beta_rm','smb':'beta_smb','hml':'beta_hml'})
betas = ff_reg_coef_unstack.iloc[:,1:].values
alpha = ff_reg_coef_unstack.iloc[:,0].values.reshape(-1)

# obtain the residuals 
residuals = df_ff.groupby('permno').apply(ff_regression,residuals=True)
var_res = residuals.groupby('permno').apply(np.var).values

```

```{python}
#| output: true
#| fig-label: fig-coefficientdistribution
#| fig-cap: "Distribution of the estimated coefficients"

ff_reg_coef = (ff_reg_coef_unstack.stack()
    .reset_index()
    .rename(columns={'level_1':'coeff',0:'value'})
    )
(
    ggplot(
        ff_reg_coef.query('coeff != "residuals"') # we don't want to plot the residuals
        ) + 
    aes(x = 'value') +
    geom_histogram(binwidth=0.05, fill='skyblue') +
    facet_wrap('coeff',scales='free') + 
    labs(x='Coefficient',y=element_blank()) +
    theme_minimal() +
    theme(
      legend_position=(0.15,0.9),
      legend_title=element_blank(),
      legend_direction='vertical',
      figure_size=(5,4),
      )
).draw()
```

Interpretation: the mean of alpha is close to 0, while the mean of all the other values are well above zero. This means that an increase to either of the $x$ variable increases so does the stock's excess return.

```{python}
# merge the estimated coeffcients to the original dataframe
df_ff_model = df_ff.merge(ff_reg_coef_unstack,on='permno',how='left')
# model the excess return for each stock
df_ff_model['ret_excess_model'] = (df_ff_model['alpha'] + 
                                   df_ff_model['beta_rm']*df_ff_model['mkt_excess'] + 
                                   df_ff_model['beta_smb']*df_ff_model['smb'] + 
                                   df_ff_model['beta_hml']*df_ff_model['hml'])
# pivot the dataframe to have the excess return for each stock in columns
df_ff_model_pivot = df_ff_model.pivot(index='month',columns='permno',values='ret_excess_model')  

# mean of all factor returns for each firm 
factor_returns = df_ff.groupby('permno')[['mkt_excess','smb','hml']].mean()

# calculate the mean and covariance matrix
mu_ff = (alpha + np.sum(betas*factor_returns,axis=1)+ 1).values 
S_ff = (betas@df_ff[['mkt_excess','smb','hml']].cov()@betas.T) + np.diag(var_res)
```

The correlation between the excess returns of two stocks depends on their factor loadings and the correlations between the factors. If two stocks have similar factor loadings, their returns will be highly correlated. Conversely, if their factor loadings differ, their returns will be less correlated.

$$\omega^* = \arg \: \max_{\Sigma_{i=1}^N w_i=1} \omega'\hat{\mu}^{FF} - \frac{\gamma}{2} \omega' \hat{\Sigma}^{FF} \omega$$
has the analytical solution
$$\omega^* = \frac{1}{\gamma} (\Sigma^{-1}  - \frac{1}{\iota \Sigma^{-1} \iota} \Sigma^{-1}\iota \iota'\Sigma^{-1} )\mu + \frac{1}{\iota \Sigma^{-1} \iota} \Sigma^{-1}$$

```{python}
# find the inverse of the estimated variance-covariance matrix
S_ff_inv = np.linalg.inv(S_ff)
# define iota: vector of ones
iota = np.ones(N)

# Calculate the numerator part
B = iota @ S_ff_inv
C = B @ iota
numerator = S_ff_inv @ mu_ff - ((B @ mu_ff - gamma) / C) * (S_ff_inv @ iota)

# Calculate the optimal weights
weights_analytical = numerator / gamma
```


```{python}
# do a check of analytical result with a numerical optimizer
obj_fun = lambda w: - ((w @ mu_ff) - (gamma / 2) * w.T @ S_ff @ w )
opt_w = minimize(
    fun=obj_fun,
    x0=weights_analytical,
    constraints=({'type': 'eq', 'fun': lambda w: np.sum(w) - 1}),
    # bounds=[(0, 1) for i in range(N)]
    options={'maxiter':1e6}
).x
if not np.allclose(opt_w,weights_analytical): print("Check your calculations again")
```


**Exercise 2**

**Bullet 1**

The parameter vector $(\theta)$ represents the sensitivity of the portfolio weights to the stock characteristics $(x_{i,t})$. In other words, $\theta$ determines how much weight is placed on each stock characteristic in determining the optimal portfolio weights. The intuition behind $\theta$ is that it captures the relationship between the characteristics of the stocks and their expected returns, thereby allowing the portfolio to be tilted towards stocks with desirable characteristics. 
<!-- 
(Her fra can blive slettet hvis plads mangel:) We note that the weight on each characteristic applies across all stocks and through time. In that way, two stocks with similar characteristics will have the same portfolio weights regardless of historical returns, which implies the assumption that $x_{i,t}$ captures all characteristics that affect returns. ((her til)) -->

Brandt, Santa-Clara, and Valkanov, 2009, estimate $\theta$ by maximizing the average utility that would have been achieved, if the portfolio had been implemented over the sample period. Specifically, they solve the unconditional optimization problem with respect to $\theta$:

$$\max_\theta \frac{1}{T} \sum_{t=0}^{T-1} u(r_{p,t+1})=\frac{1}{T} \sum_{t=0}^{T-1} u\left(\sum_{i=1}^{N_t} w_{i,t}^{PP} r_{i,t+1}\right)$$

Where they use CRRA utility throughout the paper.
<!-- 
$$\frac{1}{T} \sum_{t=0}^{T-1} h(r_{t+1}, x_t; \theta) = \frac{1}{T} \sum_{t=0}^{T-1} u'(r_{p,t+1})(x_t^Tr_{t+1}) = 0$$

And is interpreted as a method of moments estimator. The asymptotic covariance matric is:

$$
\Sigma_{\theta} \equiv \text{AsyVar}(\hat{\theta}) = \frac{1}{T} \left[ G^T V^{-1} G \right]^{-1},
$$
Where $G = \frac{1}{T} \sum_{t=0}^{T-1} \frac{\partial h(r_{t+1}, x_t; \theta)}{\partial \theta}$, and V estimates the matrix $h(r,x;\theta)$. -->

**Bullet 2**
Directly parametrizing portfolio weights reduces dimensionality exponentially compared to the two-step procedure in Problem 1. As such, the compounding of errors that can occur in the two-step procedure are minimized, reducing problems with overfitting and high variance or imprecise coefficient estimates. Furthermore, parametrizing portfolio weights, instead of the two-step procedure in Problem 1, reduces the computational requirements significantly. The computational requirements only grow with the number of characteristics and not with the number of stocks. 

However, direct weight parametrization offer less intuitive interpretations. The weight on each characteristic is constant through stocks and time, which implies that e.g. the characteristic “book-to-market ratio” has the same effect on a car manufacturing company in 1980 as it has on a tech company in 2000. Intuitively, that does not make much sense. The two-step approach in Problem 1 offer more straightforward economic interpretations.

**Bullet 3**
To estimate $\theta$, we use the same approach as in Brandt, et. al (2009). Specifically, we solve the following optimization problem:
<!-- 
We maximize the certainty equivalent $CE(\omega^{PP})$ using an approach that resembles method of moments. First, we calculate the portfolio weights $(\omega^{PP})$ for any $\theta$ (essentially just defining $\omega^{PP}_{i,t}$). Then, we plug $\omega^{PP}_{i,t}$ into $CE(\omega^{PP}_{i,t})$ and finally we maximize the function subject to $\theta$ to get $\hat{\theta}$. -->
$$\hat{\theta}=\max_\theta E(R_{r,t})-\frac{\gamma}{2}Var(R_{r,t})$$

Where $R_{p,t}=1+\sum^N_{i=1}r_{i,t}(\bar{\omega}_{i,t}+\frac{1}{N}\theta'x_{i,t})$ and $\bar{\omega}_{i,t}$ is the naive portfolio.

 The results are presented in @tbl-thetas.
```{python}
#| output: true

def calculate_naive_portfolio(N):
    """
    This function calculates the portfolio weights of the naive portfolio
        Args:
            N: the number of assets
        Returns:
            the portfolio weights
    """
    return 1/N * np.ones(N)

#Only data before 2015
df_pp = deepcopy(df_train)

r_it =  df_pp.pivot(index='month',columns='permno',values='ret_excess')

# Calculating the portfolio policy weights for at given theta
def calc_pp_weight(theta): 
    """
    This function calculates the portfolio weights for the portfolio optimization problem
        Args: 
            theta: the vector of parameters
        Returns:
            the portfolio weights
    """
    df_pp.loc[:,'x'] = (df_pp[['beta','size','bm']]*theta.T).sum(axis=1)
    return (df_pp.pivot(index='month',columns='permno',values='x')*1/N + calculate_naive_portfolio(N))

# Defining the objective function for the given certainty equivalent
def objective_function_ex2(theta):
    """
    This function calculates the objective function for the portfolio optimization problem
        Args:
            theta: the vector of parameters
        Returns:
            the value of the objective function
    """
    # calculate the portfolio weights for the given theta
    w_it = calc_pp_weight(theta)
    # calculate the return of the portfolio
    R_pt = (w_it*r_it).sum(axis=1) + 1
    # calculate the expected return as the mean of the portfolios past returns
    E_R = R_pt.mean()
    # calculate the variance of the portfolio returns
    Var_R = R_pt.var()

    return -(E_R - (gamma/2)*Var_R) # return the negative value of the objective function

# Maximizing the objective function
results = minimize(
    fun=objective_function_ex2,
    x0=np.ones(3),
    options={'maxiter':1e6}
)
theta_hat = results.x
```
```{python}
#| output: asis
#| label: tbl-thetas
#| tbl-cap: ""
# Creating a DataFrame to present theta_hat
theta_hat_df = pd.DataFrame([theta_hat], columns=['Beta', 'Size', 'Bm'], index=['Theta'])

theta_hat_df.round(3)

```


```{python}
# calculate the portfolio weights based on the optimal theta
w_pp = calc_pp_weight(theta_hat).iloc[-1,:].values

```



Our estimates show that the portfolio policy, relative to the naive portfolio, is biased towards low beta stocks, with a small market cap and a high book-to-market ratio. The characteristics are cross-sectionally standardized, and therefore the magnitudes can be compared. We see that market capitalization has the highest relative impact of the three characteristics on the portfolio policy. Small firms have been shown to outperform larger firms, and so a negative sign on the market capitalization coefficient makes sense. However, the effect might be biased in our analysis due the construction of the dataset that consists solely of selected stocks with continuous trading history from 2000-2023. As such, none of the analyzed firms went bankrupt during the period, which creates an inherent bias towards small firms. 
The negative sign of the beta characteristic coefficient aligns with the result in Frazzini & Pedersen (2014), that one should bet against beta. The intuition is that liquidity constrained investors with low risk-aversion bid up high beta assets. Lastly, the positive sign on the book-to-market characteristic coefficient is also aligned with the literature. Firms with a higher fundamental value are favored, which makes sense intuitively.

**Bullet 4**

We calculate the portfolio policy weights in December 2015 and save them for use in Problem 3.

# Exercise 3

```{python}
# Estimate the covariance matrix
def cov1Para(Y,k = None):
    """
    This function estimates the covariance matrix for the stocks in the crsp_monthly dataset
    """
    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be
    #    None, np.nan or int
    #Post-Condition: Sigmahat dataframe is returned

    # de-mean returns if required
    N,p = Y.shape                      # sample size and matrix dimension
   
    #default setting
    if k is None:
        mean = Y.mean(axis=0)
        Y = Y.sub(mean, axis=1)                               #demean
        k = 1
    #vars
    n = N-k                                    # adjust effective sample size
    
    
    #Cov df: sample covariance matrix
    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     
    
    # compute shrinkage target
    diag = np.diag(sample.to_numpy())
    meanvar= sum(diag)/len(diag)
    target=meanvar*np.eye(p)
    
    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)
    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))
    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns
    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))
    

    pihat = sum(piMat.sum())
    
    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)
    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2
    
    # compute shrinkage intensity
    kappahat=pihat/gammahat
    shrinkage=max(0,min(1,kappahat/n))
    
    # compute shrinkage estimator
    sigmahat=shrinkage*target+(1-shrinkage)*sample
    
    return sigmahat
```

```{python}
df_pf_train = df.loc[df['month']<='2015-12-31']
df_pf_test = df.loc[df['month']>'2015-12-31']
out_of_sample_r = df_pf_test.pivot(index='month',columns='permno',values='ret_excess').values
out_of_sample_sigma = np.cov(out_of_sample_r, rowvar=False)
```

```{python}
df_pf_train_pivot = df_pf_train.pivot(index='month',columns='permno',values='ret_excess')
mu_tilde = df_pf_train_pivot.mean(axis=0).values
S_tilde = cov1Para(df_pf_train_pivot).values
del df_pf_train_pivot
```

```{python}
def calculate_min_variance_portfolio(Sigma):
    """
    This function calculates the minimum variance portfolio
    """
    Sigma_inv = np.linalg.inv(Sigma)
    w_mvp = Sigma_inv @ iota    
    w_mvp = w_mvp/np.sum(w_mvp) # normalize weights
    return w_mvp

def calculate_efficient_portfolio(Sigma, mu, gamma, return_multiple = 2):
    """
    This function calculates the efficient portfolio that 
    """
    Sigma_inv = np.linalg.inv(Sigma)
    #----- minimum variance portfolio    
    mvp_weights = calculate_min_variance_portfolio(Sigma)

    #----- efficient frontier portfolio
    mu_bar = return_multiple * mvp_weights.T @ mu
    C = iota.T @ Sigma_inv @ iota
    D = iota.T @ Sigma_inv @ mu
    E = mu.T @ Sigma_inv @ mu
    lambda_tilde = 2 * (mu_bar - D/C) / (E-D**2/C)
    efp_weights = mvp_weights + lambda_tilde/2 * (Sigma_inv@mu - D* mvp_weights ) 
    return efp_weights

# create the portfolios 
naive_portfolio = calculate_naive_portfolio(mu_tilde.shape[0])
eff_portfolio = calculate_efficient_portfolio(S_tilde, mu_tilde, gamma)
ex1_portfolio = weights_analytical
ex2_portfolio = w_pp
```

```{python}
def performance_calculations(weights):
    """
    This function calculates the performance of the portfolio
        Args: 
            weights: the weights of the portfolio
        Returns:
            A dictionary with the mean, standard deviation and sharpe ratio of the portfolio
    """
    annualize_factor = 12
    annualize_return =  (out_of_sample_r @ weights).mean()*annualize_factor
    annualized_std = np.sqrt(weights @ out_of_sample_sigma @ weights) * np.sqrt(annualize_factor)
    ann_sharpe_ratio = annualize_return/annualized_std
    return {
        'Mean':annualize_return,
        'Std. Dev.': annualized_std, 
        'Sharpe Ratio' : ann_sharpe_ratio}

# create a dataframe to store the performance calculations
df_rep = pd.DataFrame({
    'Naive': performance_calculations(naive_portfolio),
    'Efficient': performance_calculations(eff_portfolio),
    'FF': performance_calculations(ex1_portfolio),
    'PP': performance_calculations(ex2_portfolio),
}).T

# create a dataframe to store the weights
df_rep_weights = pd.DataFrame({
    'Naive': naive_portfolio,
    'Efficient': eff_portfolio,
    'FF': ex1_portfolio,
    'PP': ex2_portfolio,
}
).stack().reset_index().rename(columns={'level_0':'permno','level_1':'Portfolio',0:'Weight'})

```

```{python}
# | output : true
# | label : tbl-performancereport
# | tbl-cap: "Annualized portfolio measures"

df_rep['Mean'] = df_rep['Mean'].mul(100) # to get it as percent
df_rep['Std. Dev.'] = df_rep['Std. Dev.'].mul(100) # to get it as percent
df_rep.round(2)
```

```{python}
# | output : true
# | label : fig-portfolioweights
# | fig-cap: Portfolio Weights for different portfolios
(
    ggplot(
        df_rep_weights, 
        aes(x='permno', y='Weight')
        ) + 
    geom_bar(
        stat='identity', 
        fill='skyblue') + 
    facet_wrap(
        'Portfolio',
        scales='free') +
    labs(
        x='Ticker', 
        y='Weight') +
    scale_y_continuous(
        labels=percent_format()
        ) +
    theme_minimal(

    ) + 
    theme(
          axis_text_x=element_blank(),
          axis_ticks_major_x=element_blank(),
          figure_size=(6,4), 
          )
)

```

